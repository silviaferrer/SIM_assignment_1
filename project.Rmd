---
title: "Assignment 1"
author: "Alicia Chimeno & Silvia Ferrer"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

<style type="text/css">
  body{
  font-size: 12pt;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Clear space and Libraries
```{r, include=FALSE}
# Libraries
library(tidyverse)
library(skimr)
library(FactoMineR)
library(car)
library(lmtest)
library(ISLR)
library(arm)
library(mice)
library(corrplot)
library(chemometrics)
library(dplyr)
library(caret)
library(FactoMineR)
library(corrplot)
rm(list=ls())
par(mfrow=c(1,1))


```
## Load dataset
```{r}
setwd("C:/Users/Silvia/OneDrive - Universitat Polit√®cnica de Catalunya/Escritorio/UPC/MASTER DS/1A/SIM/SIM_assignment_1")
#setwd("/Users/ali/Desktop/MASTER/SIM/PROJECT 1")
train<-read.delim("train.csv", sep=',') 
```

# Explore dataset and Preproces dataset
We start by looking at an overview of our dataset.
```{r}
dim(train)
#names(train)
str(train)
#skim(train)
```

# Preprocess and select variables
## Record incorrect NA's
```{r}
train$Alley[which(is.na(train$Alley))] <- "No alley access"
train$BsmtQual[which(is.na(train$BsmtQual))] <- "No Basement"
train$BsmtCond[which(is.na(train$BsmtCond))] <- "No Basement"
train$BsmtExposure[which(is.na(train$BsmtExposure))] <- "No Basement"
train$BsmtFinType1[which(is.na(train$BsmtFinType1))] <- "No Basement"
train$BsmtFinType2[which(is.na(train$BsmtFinType2))] <- "No Basement"
train$FireplaceQu[which(is.na(train$FireplaceQu))] <- "No Fireplace"
train$GarageType[which(is.na(train$GarageType))] <- "No Garage"
train$GarageFinish[which(is.na(train$GarageFinish))] <- "No Garage"
train$GarageCond[which(is.na(train$GarageCond))] <- "No Garage"
train$PoolQC[which(is.na(train$PoolQC))] <- "No Pool"
train$Fence[which(is.na(train$Fence))] <- "No Fence"
train$MiscFeature[which(is.na(train$MiscFeature))] <- "None"
```

## Recode variables to factor
Then,the character features and some numeric with a certain distribution are converted to factors.
```{r}
# Recode: character to factor
char_cols <- which(sapply(train, is.character))
train[, char_cols] <- lapply(train[, char_cols], as.factor) 

# Recode: numeric to factor
train$MSSubClass <- factor(train$MSSubClass)
train$MoSold <- factor(train$MoSold) # month 

```

## Target variable exploration "SalePrice"
We asses a normality test to the target variable. We reject null hypothesis (H0: variable is normal), therefore we shall not consider that the target variable is normally distributed.
```{r}
res.con <- condes(train,81)
head(res.con$quali)
shapiro.test(train$SalePrice)  
qqnorm(train$SalePrice)
qqline(train$SalePrice)
hist(train$SalePrice, prob = TRUE, col = 'lightblue', main = 'SalePrice Distribution', xlab = 'SalePrice')
#lines(density(train$SalePrice), col = 'red', lwd = 2)
```

## Selection of 10 factor variables and numerical variables 
We select the first 10th categorical variables that are more associated with the target variable.
```{r}
vect<-row.names(res.con$quali)
vect[1:10] 
factor_train<-train[,c(vect[1:10])]
num_cols <- names(train)[sapply(train, is.numeric)]
train2 <- train[,c(num_cols, vect[1:10])]
dim(train2)

```

## Variable analysis
# Analysis functions
To proceed with the predictor analysis, we do create the following functions for analyzing our numerical features:
```{r}
## numeric variable description function
numeric_description <- function(variable, n_breaks) {
  cat("Summary:\n")
  print(summary(variable))
  
  cat("\nCount of missing values:",sum(is.na(variable)),"\n")
  column_name <- sub(".+\\$", "", deparse(substitute(variable)))
  hist(variable, breaks = n_breaks, freq = F, main = paste("Histograma de", column_name), xlab = column_name)
  curve(dnorm(x, mean(variable), sd(variable)), add = T)
  
  # Normality test with Shapiro-Wilk
  print(shapiro.test(variable))
}

## analyze outliers functions
analyze_outliers <- function(data, column_name) {
  sm <- summary(data[[column_name]])
  iqr <- sm["3rd Qu."] - sm["1st Qu."]
  
  # Mild Outliers
  mild_ub <- sm["3rd Qu."] + 1.5 * iqr
  mild_lb <- sm["1st Qu."] - 1.5 * iqr
  
  mild_outliers <- length(which(data[[column_name]] > mild_ub | data[[column_name]] < mild_lb))
  cat("Number of mild outliers:", mild_outliers, "\n")
  
  # Plotting mild outliers
  Boxplot(data[[column_name]], main = paste("Outlier Analysis for", column_name),
          ylab = column_name, outline = TRUE)
  abline(h = mild_ub, col = "orange", lwd = 2)
  abline(h = mild_lb, col = "orange", lwd = 2)
  
  # Severe Outliers
  severe_ub <- sm["3rd Qu."] + 3 * iqr
  severe_lb <- sm["1st Qu."] - 3 * iqr
  severe_outliers <- length(which(data[[column_name]] > severe_ub | data[[column_name]] < severe_lb))
  cat("Number of severe outliers:", severe_outliers, "\n")
  # Plotting severe outliers
  abline(h = severe_ub, col = "red", lwd = 2.5)
  abline(h = severe_lb, col = "red", lwd = 2.5)
}
# count outliers (new column)

train2$univ_outl_count <- 0 # initialize

# count outliers
update_outliers_count <- function(dataframe, column_name) {
  df <- dataframe
  sm <- summary(df[[column_name]])
  iqr <- sm["3rd Qu."] - sm["1st Qu."]
  # Severe Outliers
  severe_ub <- sm["3rd Qu."] + 3 * iqr
  severe_lb <- sm["1st Qu."] - 3 * iqr
  severe_outliers_id <- which(df[[column_name]] > severe_ub | df[[column_name]] < severe_lb)
  df$univ_outl_count[severe_outliers_id] <- df$univ_outl_count[severe_outliers_id] + 1
  return(df)
}

```

# EDA and univariate outliers of each variable
We continue performing a descriptive analysis of each variable. Therefore, we are using the function update_outliers_count where we are updating a new variable named univ_outl_count, which is a count of the total univariate outliers an instance has, in order to track the data quality and associate having univariate outliers to some features with a correlation matrix.
The order of this descriptive analysis follows the criteria of splitting into numerical and then categorical variables.

```{r}
skim(train2) # overall exploration
#names(train2)
```

## Numerical variables
Firstly, we are analyzing the numerical variables, excluding the variable 'Id', which is not considered for the analysis.

### "LotFrontage" Linear feet of street connected to property (cont)
```{r}
numeric_description(train2$LotFrontage, 30) # not normal distribution
analyze_outliers(train2, "LotFrontage")
train2 <- update_outliers_count(train2, "LotFrontage")

```


### "LotArea" Lot size in square feet (cont)

```{r}
numeric_description(train2$LotArea, 20) # not normal distribution
analyze_outliers(train2, "LotArea")
train2 <- update_outliers_count(train2, "LotArea")

# new variable:
sm<-summary(train2$LotArea)
plot(train2$LotArea,train2$SalePrice)

iqr <- sm["3rd Qu."] - sm["1st Qu."]
mild_ub <- sm["3rd Qu."] + 1.5 * iqr
mild_lb <- sm["1st Qu."] - 1.5 * iqr

train2$f.LotArea <- ifelse(
  train2$LotArea <= mild_lb, 
  1, 
  ifelse(
    train2$LotArea > mild_lb & train2$LotArea < mild_ub, 
    2,
    ifelse(
      train2$LotArea >= mild_ub, 
      3,
      
      NA 
    )
  )
)

train2$f.LotArea<- factor(train2$f.LotArea, labels=c("LowLotArea","MidLotArea","HighLotArea"), order = T, levels=c(1,2,3))
table(train2$f.LotArea)
```

### "OverallQual" (cont)

```{r}
numeric_description(train2$OverallQual, 10) # not normal distribution
analyze_outliers(train2, "OverallQual")
train2 <- update_outliers_count(train2, "OverallQual")
```

### "YearBuilt" (int)
```{r}
summary(train2$YearBuilt)
barplot(table(train2$YearBuilt), main = "Distribution of Year Built",xlab = "Year",col = "skyblue")
sum(is.na(train2$YearBuilt))
analyze_outliers(train2, "YearBuilt")
train2 <- update_outliers_count(train2, "YearBuilt")
```

### "YearRemodAdd" (int) - Remodel date (same as construction date if no remodeling or additions)
```{r}
summary(train2$YearRemodAdd)
barplot(table(train2$YearRemodAdd), main = "Distribution of Year Remodelized",xlab = "Year",col = "skyblue")
sum(is.na(train2$YearRemodAdd))
analyze_outliers(train2, "YearRemodAdd")
train2 <- update_outliers_count(train2, "YearRemodAdd")
```


### MasVnrArea - Masonry veneer area in square feet (cont)

```{r}
numeric_description(train2$MasVnrArea, 10)# not normal distribution
analyze_outliers(train2,"MasVnrArea")
train2 <- update_outliers_count(train2, "MasVnrArea")
# find values that are NA instead of 0
# if it has a material but a 0 in masvnrarea is a NA
train2$MasVnrArea[which(train2$MasVnrArea == 0)] <- NA
none_idx <- which(train$MasVnrType == 'None')
train2$MasVnrArea[none_idx] <- 0
```


### "BsmtFinSF1" - Type 1 finished square feet (cont)

```{r}
numeric_description(train2$BsmtFinSF1, 10)# not normal distribution
analyze_outliers(train2,"BsmtFinSF1")
train2 <- update_outliers_count(train2, "BsmtFinSF1")
```

### BsmtFinSF2 - Rating of basement finished area (if multiple types) (cont)

```{r}
numeric_description(train2$BsmtFinSF2, 10) # not normal distribution
analyze_outliers(train2,"BsmtFinSF2")
train2 <- update_outliers_count(train2, "BsmtFinSF2")
```

### "BsmtUnfSF" (cont)

```{r}
numeric_description(train2$BsmtUnfSF, 20) # not normal distribution
analyze_outliers(train2,"BsmtUnfSF")
train2 <- update_outliers_count(train2, "BsmtUnfSF")
```

### "TotalBsmtSF" (cont)

```{r}
numeric_description(train2$TotalBsmtSF, 10)# not normal distribution
analyze_outliers(train2,"TotalBsmtSF")
train2 <- update_outliers_count(train2, "TotalBsmtSF")
```

### X1stFlrSF - First Floor square feet (cont)

```{r}
numeric_description(train2$X1stFlrSF, 10)
# not normal distribution
analyze_outliers(train2,"X1stFlrSF")
train2 <- update_outliers_count(train2, "X1stFlrSF")
```

### X2ndFlrSF - Second floor square feet (cont)

```{r}
numeric_description(train2$X2ndFlrSF, 10)# not normal distribution
analyze_outliers(train2,"X2ndFlrSF")
train2 <- update_outliers_count(train2, "X2ndFlrSF")

# create a new variable if has second floor = 1 / no second floor = 0. 
train2$secondfloor<-0
train2$secondfloor[which(train2$X2ndFlrSF!=0)]<-1
train2$secondfloor<-factor(train2$secondfloor)
table(train2$secondfloor) 
```

### LowQualFinSF - Low quality finished square feet (all floors) (cont)

```{r}

numeric_description(train2$LowQualFinSF, 10) # not normal distribution
table(train2$LowQualFinSF)# very centered in 0 
nearZeroVar(train2$LowQualFinSF, saveMetrics = TRUE) # Variable with near zero variance
analyze_outliers(train2,"LowQualFinSF")
train2 <- update_outliers_count(train2, "LowQualFinSF")
```

### GrLivArea - Above grade (ground) living area square feet (cont)

```{r}
numeric_description(train2$GrLivArea, 20) # not normal distribution
analyze_outliers(train2,"GrLivArea")
train2 <- update_outliers_count(train2, "GrLivArea")
```

### BsmtFullBath - # full baths in the basement (int)

```{r}
summary(train2$BsmtFullBath) 
analyze_outliers(train2,"BsmtFullBath")
train2 <- update_outliers_count(train2, "BsmtFullBath")
table(train2$BsmtFullBath)
sum(is.na(train2$BsmtFullBath))
barplot(table(train2$BsmtFullBath), main = "Distribution of Baths in the basement",xlab = "Number of Baths",col = "skyblue")
```

### BsmtHalfBath - # half baths in the basement (int)

```{r}
summary(train2$BsmtHalfBath) 
analyze_outliers(train2,"BsmtHalfBath")
train2 <- update_outliers_count(train2, "BsmtHalfBath")
table(train2$BsmtHalfBath) # very centered in 0
nearZeroVar(train2$BsmtHalfBath, saveMetrics = TRUE) # Variable not with near zero variance
sum(is.na(train2$BsmtHalfBath))
barplot(table(train2$BsmtHalfBath), main = "Distribution of Half Baths in the basement",xlab = "Number of Baths",col = "skyblue")

```

### FullBath - full baths above grade (int)

```{r}
summary(train2$FullBath) 
analyze_outliers(train2,"FullBath")
train2 <- update_outliers_count(train2, "FullBath")
table(train2$FullBath)
sum(is.na(train2$FullBath))
barplot(table(train2$FullBath), main = "Distribution of Baths above grade",xlab = "Number of Baths",col = "skyblue")
```

### HalfBath - Half baths above grade (int)

```{r}
summary(train2$HalfBath) 
analyze_outliers(train2,"HalfBath")
train2 <- update_outliers_count(train2, "HalfBath")
table(train2$HalfBath)
sum(is.na(train2$HalfBath))
barplot(table(train2$HalfBath), main = "Distribution of Half Baths above grade",xlab = "Number of Baths",col = "skyblue")
```

### BedroomAbvGr - Bedrooms above grade (does NOT include basement bedrooms)

```{r}
summary(train2$BedroomAbvGr) 
analyze_outliers(train2,"BedroomAbvGr")
train2 <- update_outliers_count(train2, "BedroomAbvGr")
table(train2$BedroomAbvGr)
sum(is.na(train2$BedroomAbvGr))
barplot(table(train2$BedroomAbvGr), main = "Distribution of Bedrooms above grade",xlab = "Number of Bedrooms",col = "skyblue")
```

### KitchenAbvGr - Kitchens above grade (int)

```{r}
summary(train2$KitchenAbvGr) 
analyze_outliers(train2,"KitchenAbvGr")
train2 <- update_outliers_count(train2, "KitchenAbvGr")
table(train2$KitchenAbvGr) # variable very centered in 1
nearZeroVar(train2$KitchenAbvGr,saveMetrics = TRUE) # near zero variance
sum(is.na(train2$KitchenAbvGr))
barplot(table(train2$KitchenAbvGr), main = "Distribution of Kitchens",xlab = "Number of Kitchens",col = "skyblue")
```

### "TotRmsAbvGrd" - Total rooms above grade (does not include bathrooms) - continuous ratio variable (int)

```{r}
summary(train2$TotRmsAbvGrd)
analyze_outliers(train2,"TotRmsAbvGrd")
train2 <- update_outliers_count(train2, "TotRmsAbvGrd")
#table(train2$TotRmsAbvGrd)
sum(is.na(train2$TotRmsAbvGrd))

```

### "Fireplaces" - Number of fireplaces (int)

```{r}
summary(train2$Fireplaces)
table(train2$Fireplaces)
sum(is.na(train2$Fireplaces)) #0
analyze_outliers(train2, "Fireplaces")
train2 <- update_outliers_count(train2, "Fireplaces")
barplot(table(train2$Fireplaces), main = "Distribution of fireplaces",xlab = "Number of Fireplaces",col = "skyblue")
```

### "GarageCars" - Size of garage in car capacity (int)

```{r}
summary(train2$GarageCars)
table(train2$GarageCars)
sum(is.na(train2$GarageCars)) #0
analyze_outliers(train2, "GarageCars") 
train2 <- update_outliers_count(train2, "GarageCars")
barplot(table(train2$GarageCars), main = "Distribution of Garage Cars",xlab = "Number of Garage Cars",col = "skyblue")
```

### "GarageArea" - Size of garage in square feet (cont)

```{r}
numeric_description(train2$GarageArea, 30)
# not normal distribution
analyze_outliers(train2, "GarageArea")
train2 <- update_outliers_count(train2, "GarageArea")
sm<-summary(train2$GarageArea)

# checking if 0 mean that they don't have garage
which(train$GarageCars == 0)
which(train$GarageArea == 0)
which(train$GarageFinish=='No Garage')
```

### "WoodDeckSF" - Wood deck area in square feet - continuous ratio variable

```{r}
numeric_description(train2$WoodDeckSF, 30) # not normal distribution
#table(train2$WoodDeckSF) # centered data in 0
analyze_outliers(train2, "WoodDeckSF")
train2 <- update_outliers_count(train2, "WoodDeckSF")
sm<-summary(train2$WoodDeckSF)
```

### "OpenPorchSF" - Open porch area in square feet - continuous ratio variable (continua)

```{r}
numeric_description(train2$OpenPorchSF, 30)
analyze_outliers(train2,"OpenPorchSF")
train2 <- update_outliers_count(train2, "OpenPorchSF")
summary(train2$OpenPorchSF)
#table(train2$OpenPorchSF) # very centered data in 0
sum(is.na(train2$OpenPorchSF))
barplot(table(train2$OpenPorchSF), main = "Distribution of ",xlab = "Number of ",col = "skyblue")
sm<-summary(train2$OpenPorchSF)
train2$OpenPorch_binary<-0
train2$OpenPorch_binary[which(train2$OpenPorchSF!=0)]<-1
train2$OpenPorch_binary<-as.factor(train2$OpenPorch_binary)
table(train2$OpenPorch_binary)

```

### EnclosedPorch - Enclosed porch area in square feet (cont)

```{r}
summary(train2$EnclosedPorch)
table(train2$EnclosedPorch) # very centered data in 0
sum(is.na(train2$EnclosedPorch))
hist(train2$EnclosedPorch, breaks = 30, freq = F)
# therefore we will create a factor 0 if they dont have enclosed porch area,1 if they do have.
train2$EnclosedPorch_binary<-0
train2$EnclosedPorch_binary[which(train2$EnclosedPorch!=0)]<-1
train2$EnclosedPorch_binary<-as.factor(train2$EnclosedPorch_binary)
#table(train2$EnclosedPorch_binary)

# create new variable - has porch or not - mixing both binary variables
train2$HasPorch_binary<-0
train2$HasPorch_binary[which(train2$EnclosedPorch!=0 | train2$OpenPorchSF!=0)]<-1
train2$HasPorch_binary<-as.factor(train2$HasPorch_binary)
table(train2$HasPorch_binary)
```

### X3SsnPorch - Three season porch area in square feet (cont)

```{r}
numeric_description(train2$X3SsnPorch, 30)
#table(train2$X3SsnPorch) # very centered in 0 -> dont have three season porch
analyze_outliers(train2,"X3SsnPorch")
train2 <- update_outliers_count(train2, "X3SsnPorch")

```

### ScreenPorch - Screen porch area in square feet (cont)

```{r}
numeric_description(train2$ScreenPorch, 30) # not normally distributed
#table(train2$ScreenPorch) # very centered in 0 -> dont have screen porch
analyze_outliers(train2,"ScreenPorch")
train2 <- update_outliers_count(train2, "ScreenPorch")
```

### PoolArea - Pool area in square feet (cont)

```{r}
numeric_description(train2$PoolArea, 30)
# not normally distributed
table(train2$PoolArea) # very centered in 0 -> dont have pool
analyze_outliers(train2,"PoolArea")
train2 <- update_outliers_count(train2, "PoolArea")
# create a new variable : 0=no pool/1= pool
train2$Pool_binary<-0
train2$Pool_binary[which(train2$PoolArea!=0)]<-1
train2$Pool_binary<-as.factor(train2$Pool_binary)
table(train2$Pool_binary) # not very usefull , the majority dont have pools, very centered
#train2$Pool_binary<-NULL
```

### MiscVal - $Value of miscellaneous feature (cont)

```{r}
numeric_description(train2$MiscVal, 30)
# not normally distributed
#table(train2$MiscVal) # very centered in 0 
analyze_outliers(train2,"MiscVal")
train2 <- update_outliers_count(train2, "MiscVal")

```

### YrSold - (int)
```{r}
barplot(table(train2$YrSold), main = "Distribution of YrSold",xlab = "Number of Garage Cars",col = "skyblue")
table(train2$YrSold) 
analyze_outliers(train2,"YrSold")
train2 <- update_outliers_count(train2, "YrSold")
```

## Categorical Variables
Categorical variables are analyzed.

### Neighborhood 

```{r, warning=FALSE}
sum(is.na(train2$Neighborhood)) #0
table(train2$Neighborhood)
barplot(table(train2$Neighborhood), main = "Distribution of Neighborhood",xlab = "Number of Neighborhood",col = "skyblue")
pie(table(train2$Neighborhood), main = "Distribution of Neighborhood", col = rainbow(length(levels(train2$Neighborhood))))

# analysis with the target variable
#ggplot(train2, aes(x=SalePrice, fill=Neighborhood)) +geom_density(alpha=.5)
ggplot(train2, aes(y=SalePrice, fill=Neighborhood)) +
 geom_boxplot(alpha=0.5)
#pairwise.wilcox.test(train2$SalePrice, train2$Neighborhood)

```

### ExterQual - Evaluates the quality of the material on the exterior 

```{r}
sum(is.na(train2$ExterQual)) #0
table(train2$ExterQual)
barplot(table(train2$ExterQual), main = "Distribution of the exterior material quality",xlab = "Qualification",col = "skyblue")

# analysis with the target variable
#ggplot(train2, aes(x=SalePrice, fill=ExterQual)) +geom_density(alpha=.5)
ggplot(train2, aes(y=SalePrice, fill=ExterQual)) +geom_boxplot(alpha=0.5)
#pairwise.wilcox.test(train2$SalePrice, train2$ExterQual)
```

### BsmtQual - Evaluates the height of the basement

```{r}
sum(is.na(train2$BsmtQual)) #0
table(train2$BsmtQual)
barplot(table(train2$BsmtQual), main = "Distribution of Basement height Quality",xlab = "Qualification",col = "skyblue")
#pie(table(train2$BsmtQual), main = "Distribution of BsmtQual", col = rainbow(length(levels(train2$BsmtQual))))

# analysis with the target variable
#ggplot(train2, aes(x=SalePrice, fill=BsmtQual)) +geom_density(alpha=.5)
ggplot(train2, aes(y=SalePrice, fill=BsmtQual)) + geom_boxplot(alpha=0.5)
#pairwise.wilcox.test(train2$SalePrice, train2$BsmtQual)
```

### KitchenQual - Kitchen quality

```{r}
sum(is.na(train2$KitchenQual)) #0
table(train2$KitchenQual)
barplot(table(train2$KitchenQual), main = "Distribution of Kitchen Quality",xlab = "Qualification",col = "skyblue")

# analysis with the target variable
#ggplot(train2, aes(x=SalePrice, fill=KitchenQual)) +geom_density(alpha=.5)
ggplot(train2, aes(y=SalePrice, fill=KitchenQual)) + geom_boxplot(alpha=0.5)
#pairwise.wilcox.test(train2$SalePrice, train2$KitchenQual)
#ks.test(train2$SalePrice[train2$KitchenQual=="TA"],train2$SalePrice[train2$KitchenQual=="Fa"]) # test if we can join certain categoris together to make a easier analysis ->rejected
```

### GarageFinish - Interior finish of the garage

```{r}
sum(is.na(train2$GarageFinish)) #0
table(train2$GarageFinish)
barplot(table(train2$GarageFinish), main = "Garage Finish Distribution",xlab = "Type",col = "skyblue")

# analysis with the target variable
#ggplot(train2, aes(x=SalePrice, fill=GarageFinish)) +geom_density(alpha=.5)
ggplot(train2, aes(y=SalePrice, fill=GarageFinish)) +
 geom_boxplot(alpha=0.5)
#pairwise.wilcox.test(train2$SalePrice, train2$GarageFinish)
```

### FireplaceQu - Fireplace quality

```{r}
sum(is.na(train2$FireplaceQu)) #0
table(train2$FireplaceQu)
barplot(table(train2$FireplaceQu), main = "Fireplace quality Distribution",xlab = "Qualification",col = "skyblue")

# analysis with the target variable
#ggplot(train2, aes(x=SalePrice, fill=FireplaceQu)) +geom_density(alpha=.5)
ggplot(train2, aes(y=SalePrice, fill=FireplaceQu)) +
 geom_boxplot(alpha=0.5)
#pairwise.wilcox.test(train2$SalePrice, train2$FireplaceQu)
```

### Foundation - Type of foundation

```{r}
sum(is.na(train2$Foundation)) #0
table(train2$Foundation)
barplot(table(train2$Foundation), main = "Type of foundation Distribution",xlab = "Type",col = "skyblue")

# analysis with the target variable
#ggplot(train2, aes(x=SalePrice, fill=Foundation)) +geom_density(alpha=.5)
ggplot(train2, aes(y=SalePrice, fill=Foundation)) +
 geom_boxplot(alpha=0.5)
#pairwise.wilcox.test(train2$SalePrice, train2$Foundation)

```

### GarageType - Garage location

```{r}
sum(is.na(train2$GarageType)) #0
table(train2$GarageType)
barplot(table(train2$GarageType), main = "Garage Location Distribution",xlab = "Garage Location",col = "skyblue")

# analysis with the target variable
#ggplot(train2, aes(x=SalePrice, fill=GarageType)) +geom_density(alpha=.5)
ggplot(train2, aes(y=SalePrice, fill=GarageType)) +
 geom_boxplot(alpha=0.5)
#pairwise.wilcox.test(train2$SalePrice, train2$GarageType)

```

### MSSubClass

```{r}
sum(is.na(train2$MSSubClass)) #0
table(train2$MSSubClass) # +classes
barplot(table(train2$MSSubClass), main = "Garage Location Distribution",xlab = "Garage Location",col = "skyblue")

# analysis with the target variable
#ggplot(train2, aes(x=SalePrice, fill=MSSubClass)) +geom_density(alpha=.5)
ggplot(train2, aes(y=SalePrice, fill=MSSubClass)) +
 geom_boxplot(alpha=0.5)
#pairwise.wilcox.test(train2$SalePrice, train2$MSSubClass)

```

## Missing values and imputation

Checking which variables have NA's we can see that LotFrontage, MasVnrArea and GarageYrBlt are the ones with missing values.
```{r}
#skim(train2)
sum(is.na(train2$LotFrontage))
sum(is.na(train2$MasVnrArea))
sum(is.na(train2$GarageYrBlt))
```

We decided to exclude the imputation of the GarageYrBlt feature because it has no sense to impute this variable, as it is a year. 
```{r}
mice_imp<-mice(train2[, !names(train2) %in% "GarageYrBlt"],method = "cart")
imputed_data<-complete(mice_imp)
```

We proceed to validate the imputation.

```{r}
# LotFrontage validation
summary(imputed_data$LotFrontage)
summary(train2$LotFrontage)
plot(density(train2$LotFrontage,na.rm=TRUE))
plot(density(imputed_data$LotFrontage,na.rm=TRUE))
```

Checking the results we can conclude that the imputation doesn't have a significant change in the density nor the summary. 

```{r}
# MasVnrArea validation
summary(imputed_data$MasVnrArea)
summary(train2$MasVnrArea)
plot(density(train2$MasVnrArea,na.rm=TRUE))
plot(density(imputed_data$MasVnrArea,na.rm=TRUE))
```

# Data Quality Exploration

To check the quality of the dataset we have a count on each individual of the univariate outliers. We can see that there are 7 instances that have 4 or more univariate outliers, therefore are candidates to delete from the analysis. 

```{r}
max(imputed_data$univ_outl_count) 
ind_delete<-imputed_data[which(imputed_data$univ_outl_count > 3),]
imputed_data<-imputed_data[-c(ind_delete)$Id,]
```

## Correlation Matrix

The correlation matrix shows a moderate negative correlation to the BsmtUnfSF (then the less Unfinished square feet of basement area, the more univ outliers) and a significant positive correlation to both BsmtFinSF2, Type 2 finished square feet, and ScreenPorch.
```{r}
cols <- num_cols[! num_cols %in% c("Id","GarageYrBlt")]
df_of_interest <- imputed_data[,c(cols,"univ_outl_count")]
cor_outl = cor(df_of_interest)
par(mfrow=c(1,1))
corrplot(cor_outl, method = 'color')

cor_outl[34, ]
colnames(cor_outl)[which.min(cor_outl[34, ])];min(cor_outl[34, ])
colnames(cor_outl)[which.max(cor_outl[34,1:33])];max(cor_outl[34,1:33])

```


## Multivariate outliers

We filter variables in order to have only those with similar distribution of values. For that reason, we have filtered the values that had more unique values than 40. Then, we have excluded the variables that were centered in the '0' value.
We are not taking off these outliers because the length is 44 and taking into account the dimensions of our dataset, 1460, it would be a reduction of the 3%.
Also, we have performed other Moutliers in order to see different scenarios that we consider rare data.

```{r}
# 1. All variable with similar high unique values
n_cols <- names(imputed_data)[sapply(imputed_data, is.numeric)]
n_cols <- n_cols[n_cols != "Id"]
df_of_interest <- imputed_data[,c(n_cols)]

threshold <- 40
df_cols <- sapply(df_of_interest, function(x) length(unique(x)) > threshold)
df_of_interest <- df_of_interest[, df_cols]

names(df_of_interest)

res.out = Moutlier(df_of_interest[, !(names(df_of_interest) %in% c("MasVnrArea","BsmtFinSF1", "BsmtFinSF2", "X2ndFlrSF", "WoodDeckSF", "EnclosedPorch", "ScreenPorch"))], quantile = 0.9995, col="green")

which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff))
length(which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff))) #44
par(mfrow=c(1,1))
plot( res.out$md, res.out$rd )
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")

# take off m outliers
#imputed_data <- imputed_data[-which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff)),]

# 2. Generally the houses built latetly are the ones with most quality
res.out <- Moutlier(imputed_data[,c("YearBuilt","OverallQual")], quantile = 0.9995, col="green")
which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff))
par(mfrow=c(1,1))
plot( res.out$md, res.out$rd )
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")

# 3. Generally the houses with more area are the ones with more rooms
res.out <- Moutlier(imputed_data[,c("GrLivArea","TotRmsAbvGrd")], quantile = 0.9995, col="green")
which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff))
par(mfrow=c(1,1))
plot( res.out$md, res.out$rd )
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")

# 4. Generally the houses with more total area are the ones with more garage area also
res.out <- Moutlier(imputed_data[,c("GarageArea","TotalBsmtSF")], quantile = 0.9995, col="green")
which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff))
par(mfrow=c(1,1))
plot( res.out$md, res.out$rd )
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")

# 5. Moutliers about total areas zones
res.out <- Moutlier(imputed_data[,c("X1stFlrSF","TotalBsmtSF","GrLivArea")], quantile = 0.9995, col="green") #GarageArea, si se lo sumo hay menos outliers
which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff))
par(mfrow=c(1,1))
plot( res.out$md, res.out$rd )
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")

```

# Profiling
As we have declared before in the target variable exploration, the distribution is not normal. To test for autocorrelation the acf() function is used.

```{r}
acf(imputed_data$SalePrice)

#duplicate of the section target exploration
shapiro.test(imputed_data$SalePrice)  ## Normality test of the target variable  ->>>  # We test the null hypothesis H0: variable is normal, H1: variable not normal.  # The p-value is less than 0.05, therefore we reject H0.  # For that we will not consider the target variable as normally distributed

ks.test(imputed_data$SalePrice, 'pnorm', mean(imputed_data$SalePrice), sd(imputed_data$SalePrice))

ggplot(data=imputed_data, aes(SalePrice, y = after_stat(density))) +
  geom_histogram(breaks=seq(0, max(imputed_data$SalePrice), by=5000),
                col='lightblue',
                fill='steelblue') +
  geom_density(lwd=1,
                col='red') +
  labs(title="Histogram for price with density", x="SalePrice", y="Count")

```

## Variables with most association with the response variable

For the quantitative variables there are some variable that are highly significant positively correlated (r > 0.50, p ~ 0) to the target variable, SalesPrice:  OverallQual, GrLivArea, GarageCars, GarageArea, TotalBsmtSF, X1stFlrSF, FullBath, TotRmsAbvGrd, YearBuilt and YearRemodAdd. This seems logical as the higher the overall condition rating of the house is and the bigger the area above grade living, the more it would cost. There isn't any variable which is highly significant negatively correlated to the SalePrice (r < -0.50 p = 0).

For the qualitative variables it is clear that the Neighborhood explains the most variance in the SalePrice variable (R^2 = 0.546, p ~ 0).  
The influence of the other qualitative variables (with R^2 > 0.3) are in order: Neighborhood, ExterQual, BsmtQual, KitchenQual, f.GarageArea, GarageFinish. EnclosedPorch_binary, Secondfloor and Pool_binary are poorly associated as they have a R^2-value under 4%.

```{r}
res.con <- condes(imputed_data,35) #names(imputed_data)[35] - [1] "SalePrice"
res.con$quanti

#done before for selecting
res.con$quali

```

## Dependence of the average SalesPrice and the level of ExterQual

With the ExterQual factor we want to argue if the average SalesPrice depends on the level of ExterQual.

We used pairwise Wilcoxon tests to compare means, and from the boxplot and distribution plot, it was evident they weren't going to be similar. The Wilcoxon test results confirmed our hypothesis, showing a clear difference between the means of the different quartiles.

```{r, warning=false}
pairwise.wilcox.test(imputed_data$SalePrice, imputed_data$ExterQual)

```

# Modeling

To create a model, first we split the train data (imputed_data) into train_new and test_new. We need the test_new to validate the accuracy of the model with data that has the target variable and the model has not used. To check if the model has overfitting.

```{r}
set.seed(123)
rows <- sample(nrow(imputed_data), .7 * nrow(imputed_data))
train_new  <- imputed_data[rows, ]
test_new <- imputed_data[-rows, ]

```

Before building the model, we should first study the multicolinearity and check if there are any predictors that are highly correlated. 
We have seen that the previous variables are very correlated, GarageArea with GarageCars with 88% correlation,  TotRmsAbvGrd and GrLivArea with 83% correlation,  X1stFlrSF and TotalBsmtSF with 81% correlation and SalePrice with OverallQual with 79% what is indeed our target variable. We will not, for now, in the modeling process consider excluding some of the variables that are very correlated.  

```{r}
num_cols <- names(train_new)[sapply(train_new, is.numeric)]
num_data<-train_new[,c(num_cols)]
descrCor <-  cor(num_data)
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > .99) 

max_cor_value <-max(abs(descrCor[upper.tri(descrCor)])) # maximum correlated
which(descrCor == max_cor_value, arr.ind = TRUE) # garageArea and garagecars with 88% correlation


vec <- as.vector(descrCor)
#sort(unique(vec), decreasing = TRUE)
sort(unique(vec), decreasing = TRUE)[3];which(descrCor == sort(unique(vec), decreasing = TRUE)[3], arr.ind = TRUE)
sort(unique(vec), decreasing = TRUE)[4];which(descrCor == sort(unique(vec), decreasing = TRUE)[4], arr.ind = TRUE) 
sort(unique(vec), decreasing = TRUE)[5];which(descrCor == sort(unique(vec), decreasing = TRUE)[5], arr.ind = TRUE) 


```

## Build different models with their metrics
We start by building the model with a manual stepwise technique, we will check the R^2, RSS and AIC metrics to select the better model. We can start by adding the more correlated numerical variables to the target variable. 

```{r echo = T, results = 'hide'}
display(m0 <- lm(SalePrice ~ 1, data = train_new))
display(m0 <- lm(log(SalePrice) ~ 1, data = train_new))
display(m1 <- lm(log(SalePrice) ~ OverallQual , data = train_new))
#plot(m1)
anova(m0,m1) # rss
as.matrix(AIC(m0, m1)) 

display(m2 <- lm(log(SalePrice) ~ OverallQual*log(GrLivArea), data = train_new)) # with interaction
display(m2 <- lm(log(SalePrice) ~ OverallQual+log(GrLivArea), data = train_new)) # without interaction
#plot(m2)
anova(m1,m2)
as.matrix(AIC(m1, m2))

display(m3 <- lm(log(SalePrice) ~OverallQual+log(GrLivArea)*GarageCars , data = train_new))
display(m3 <- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars , data = train_new))
anova(m2,m3) 
as.matrix(AIC(m2, m3)) 
summary(m3)

display(m4 <- lm(log(SalePrice)~OverallQual+log(GrLivArea)+GarageCars+GarageArea , data = train_new))
anova(m3,m4)
as.matrix(AIC(m3, m4))
summary(m4)

display(m5<- lm(log(SalePrice) ~OverallQual +log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF , data = train_new))
anova(m4,m5)
summary(m5) 
as.matrix(AIC(m4, m5)) 

display(m6<- lm(log(SalePrice) ~OverallQual +log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+ X1stFlrSF, data = train_new))
anova(m5,m6)
summary(m6) 
as.matrix(AIC(m5, m6))  #does not improve

display(m7<- lm(log(SalePrice) ~OverallQual +log(GrLivArea)+GarageCars+TotalBsmtSF+FullBath, data = train_new))
anova(m5,m7)  # does not improve much
summary(m7) 
as.matrix(AIC(m5, m7)) 

display(m8<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+TotRmsAbvGrd, data = train_new))
anova(m5,m8) # not improve
as.matrix(AIC(m7, m8)) 

display(m9<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt, data = train_new))
anova(m5,m9) 
summary(m8)
as.matrix(AIC(m8, m9))  # improves

display(m10<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd, data = train_new))
anova(m9,m10) 
summary(m10)
as.matrix(AIC(m9, m10)) 

display(m11<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+MasVnrArea, data = train_new))
anova(m10,m11) 
summary(m11)
as.matrix(AIC(m10, m11))  # not much better


display(m12<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces, data = train_new))
anova(m10,m12) 
summary(m12) # r^2 better
as.matrix(AIC(m10, m12)) 


display(m13<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1, data = train_new))
anova(m12,m13) 
summary(m13) # r^2 NOT better , rss and AIC yes -> keep
as.matrix(AIC(m10, m13))

display(m14<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1+LotFrontage, data = train_new))
anova(m13,m14) # R^2 bbetter -> keep
summary(m14)
as.matrix(AIC(m13, m14))

display(m15<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1+LotFrontage+WoodDeckSF, data = train_new))
anova(m14,m15) 
summary(m15) 
as.matrix(AIC(m14, m15))

# delete garagearea bv very correlated to garage
display(m15<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1+LotFrontage+WoodDeckSF, data = train_new))
anova(m14,m15) 
summary(m15) 
as.matrix(AIC(m14, m15))

```


We can also use the automatic stepwise() and see which are the variables that will create a model with higher accuracy.

```{r}
mod.fow <- stats::step(lm(SalePrice~. , data = train_new), trace = FALSE,
                       direction = "forward")
summary(mod.fow)

# Models: 

n00<- lm(log(SalePrice) ~ LotArea + OverallQual + OverallCond + YearBuilt +
                  MasVnrArea + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF +
                  X1stFlrSF + X2ndFlrSF  +
                  BsmtFullBath + FullBath + HalfBath + BedroomAbvGr +
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  OpenPorchSF + X3SsnPorch + ScreenPorch +
                  PoolArea, data = train_new)
#summary(n00)
summary(n00)$r.squared
par(mfrow=c(2,2))
plot(n00)
par(mfrow=c(1,1))
influencePlot(n00)
corrplot(descrCor, method = 'color')

# check if there are any correlated predictors
treshold_vif<-function(rsq){
  vif<- 1/(1-rsq)
  return(vif)
}

treshold_vif(summary(n00)$r.squared)
vif(n00) # there are not
which(as.vector(vif(n00))>treshold_vif(summary(n00)$r.squared))# there are not highly correlated variables in the model

# delete with a backwards stepwise : MasVnrArea, FullBath, HalfBath, BedroomAbvGr,OpenPorchSF, X3SsnPorch
n0<- lm(log(SalePrice) ~  LotArea + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea, data = train_new)
```

##  Plot to show which transformations we shall apply
```{r}
# Check BoxTidwell tests: 
# We guess that the variables with a higher range will have a logaritmic transformation (the area variables)

n0<- lm(log(SalePrice) ~  LotArea + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea, data = train_new)

# we do BoxTidwall test to see which transformations we shall apply in the predictors to make a better model. 
# LotArea
boxTidwell(log(SalePrice) ~ LotArea, ~OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea,data=train_new, max.iter = 70 )

v0<- lm(log(SalePrice) ~  log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea, data = train_new)
# we shall convert the LotArea variable into logaritmic

# X1stFlrSF
boxTidwell(log(SalePrice) ~ X1stFlrSF,~ log(LotArea)+ OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+ X2ndFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea,data=train_new, max.iter = 70 )

v1<- lm(log(SalePrice) ~  log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+ log(X1stFlrSF)+ X2ndFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea, data = train_new)


# X2stFlrSF

boxTidwell(log(SalePrice) ~ I(X2ndFlrSF+0.01),~ log(LotArea)+ OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+X1stFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea,data=train_new, max.iter = 70 ) # we dont need


boxTidwell(log(SalePrice) ~ I(PoolArea+0.01),~ log(LotArea)+ OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+X1stFlrSF + X2ndFlrSF+
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch,data=train_new, max.iter = 70 ) # no either


#summary(v1)
summary(v1)$r.squared
par(mfrow=c(2,2))
plot(v1)
par(mfrow=c(1,1))
#influencePlot(v1)
which(as.vector(vif(v1))>treshold_vif(summary(v1)$r.squared))# there are not highly correlated variables in the model

```

## Select the best model
Therefore we have 2 potential models. We have been looking on the accuracy of the model, currently on 87% (will depend on the seed), but we need to check if there is overfitting using the test sample. 
Looking at these results, seems that the model has no overfitting. No significant difference on the measures of the model between the train sample and the test.

```{r}
rmse <- function(fitted, actual){
  sqrt(mean((fitted - actual)^2))
}

RSQUARE <- function(predictions, actual_values) {
  ss_residual <- sum((actual_values - predictions)^2)
  ss_total <- sum((actual_values - mean(actual_values))^2)
  rsquare <- 1 - (ss_residual / ss_total)
  return(rsquare)
}


results <- data.frame(Model = c("Model with train_new sample",
                                "Model with test_new sample"),
                      RSQRT = round(c(RSQUARE(exp(fitted(v1)), train_new$SalePrice),
                               RSQUARE(exp(predict(v1, newdata = test_new)), test_new$SalePrice)),2),
                      RMSE = round(c(rmse(exp(fitted(v1)), train_new$SalePrice),
                               rmse(exp(predict(v1, newdata = test_new)), test_new$SalePrice)),2))
results

```

## Add categorical variables to the model
Now we shall proceed to adding the categorical variables for our model creation. We now add to the model 
```{r}
#names(train_new)
b0<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea + Neighborhood
        , data = train_new)
#summary(b0)
summary(b0)$r.squared
anova(v1,b0)
AIC(v1,b0)
treshold_vif(summary(b0)$r.squared)
vif(b0) # High vif-> Highly correlated predictor -> wont add to the model


b1<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual
        , data = train_new)

#summary(b1)
summary(b1)$r.squared
anova(n0,b1)
AIC(n0,b1)
treshold_vif(summary(b1)$r.squared)
vif(b1)
# check interactions
b11<- lm(log(SalePrice) ~ (LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea)*ExterQual
        , data = train_new)
#summary(b11)
summary(b11)$r.squared
anova(b1,b11) # better with interactions
AIC(b1,b11)


b2<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual
        , data = train_new)

#summary(b2)
summary(b2)$r.squared
anova(b1,b2)
AIC(b1,b2)
treshold_vif(summary(b2)$r.squared)
vif(b2)
# interactions
b21<- lm(log(SalePrice) ~ (LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual) *BsmtQual
        , data = train_new)
#summary(b21)
summary(b21)$r.squared
anova(b2,b21) #  better with interaction 
AIC(b2,b21)
anova(b11,b21) # but worse than b11


b3<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual
        , data = train_new)

#summary(b3)
summary(b3)$r.squared
anova(b2,b3)
AIC(b2,b3)
treshold_vif(summary(b3)$r.squared)
vif(b3)
#with interactions
b31<- lm(log(SalePrice) ~ (LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual)* KitchenQual
        , data = train_new)

#summary(b31)
summary(b31)$r.squared
anova(b3,b31) #  better model with interactions 
AIC(b3,b31)



b4<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+ GarageFinish
        , data = train_new)

#summary(b4)
summary(b4)$r.squared
anova(b3,b4) # not better model -> we dont use this variable
AIC(b3,b4)
treshold_vif(summary(b4)$r.squared)
vif(b4)

b5<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual + FireplaceQu
        , data = train_new)

#summary(b5)
summary(b5)$r.squared
anova(b3,b5) # not better model
AIC(b3,b5)
treshold_vif(summary(b5)$r.squared)
vif(b5)

b6<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+Foundation
        , data = train_new)

#summary(b6)
summary(b6)$r.squared
anova(b3,b6)
AIC(b3,b6)
treshold_vif(summary(b6)$r.squared)
vif(b6) # we dont add it because highly correlated

b7<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType
        , data = train_new)

#summary(b7)
summary(b7)$r.squared
anova(b3,b7)
AIC(b3,b7)
treshold_vif(summary(b7)$r.squared)
vif(b7) 
# interactions 
b71<- lm(log(SalePrice) ~ (LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea)*GarageType +ExterQual + BsmtQual + KitchenQual
        , data = train_new)

#summary(b71)
summary(b71)$r.squared
anova(b7,b71) # better model 
AIC(b7,b71)
treshold_vif(summary(b71)$r.squared)

b8<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType+MSSubClass
        , data = train_new)

#summary(b8)
summary(b8)$r.squared
anova(b7,b8) # not better model
AIC(b7,b8)
treshold_vif(summary(b8)$r.squared)
vif(b8) # highly correlated


b9<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType+BsmtFinType1
        , data = train_new)

#summary(b9)
summary(b9)$r.squared
anova(b7,b9)  # not better model
AIC(b7,b9)
treshold_vif(summary(b9)$r.squared)


AIC(b7,b71,b3,b31,b2,b21,b1,b11) # b31 better but too many df, -> we keep b3

# now we have done a forward stepwise, we can see if we need all the variables doing a backwards stepwise, we can see that in the model: LotFrontage PoolArea and GarageType are less influental, for better explainability we can not consider it in our model.
final_model<-b3

```

## Final model metrics

```{r}
results <- data.frame(Model = c("Model with train_new sample",
                                "Model with test_new sample"),
                      RSQRT = round(c(RSQUARE(exp(fitted(final_model)), train_new$SalePrice),
                               RSQUARE(exp(predict(final_model, newdata = test_new)), test_new$SalePrice)),2),
                      RMSE = round(c(rmse(exp(fitted(final_model)), train_new$SalePrice),
                               rmse(exp(predict(final_model, newdata = test_new)), test_new$SalePrice)),2))
results
```

## Study the presence of a priori influential data observations
In this case, 26 a priori values where found.

```{r}
mean_hat <- mean(hatvalues(final_model));mean_hat
priori <- which(hatvalues(final_model)>4*mean_hat)
length(priori)
```
## Study the presence of a posteriori influential values

For studying the posteriori influential values, we have looked to cook‚Äôs distance outliers, where 3 influent data are found and are removed from the sample.
The best model is then reconstructed on this new data set and compared with its original to see the difference. 
Comparing the previous coefficients and the current coefficients, we can say that the change has no relevant changes in the coefficients. 
The R-squared of the model has increased up to 0.94.

```{r}
betas <- as.data.frame(dfbetas(final_model))
betas_cutoff <- 2 / sqrt(dim(train_new)[1]);betas_cutoff

influencePlot(final_model, id=list(n=3, method="noteworthy"))

```
```{r, warning=false}
llcoo <-c("1424","811","1183")
# residual outliers
llres <- which(abs(rstudent(final_model))>qnorm(0.995));length(llres)
llrem <- unique(c(rownames(train_new)[llres],llcoo)); length(llrem)
llremreg<-which(rownames(train_new)%in%llrem);llremreg
train_new<-train_new[-llremreg,]

final_modelp <- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType
        , data = train_new)
summary(final_modelp)
final_modelp <- step( final_modelp, k=log(nrow(train_new)))

par(mfrow=c(2,2))
plot(final_modelp)


coef_orig <- coef(final_model)
coef_filt <- coef(final_modelp)

#common variables
var_com <- intersect(names(coef_orig), names(coef_filt))

coef_orig <- coef_orig[var_com]
coef_filt <- coef_filt[var_com]

coef_comparison <- data.frame(Variable = names(coef_orig),
                               Original_coefficient = coef_orig,
                               Filtered_coefficient = coef_filt)
print(coef_comparison)


results <- data.frame(Model = c("Model with train_new sample",
                                "Model with test_new sample"),
                      RSQRT = round(c(RSQUARE(exp(fitted(final_modelp)), train_new$SalePrice),
                               RSQUARE(exp(predict(final_modelp, newdata = test_new)), test_new$SalePrice)),2),
                      RMSE = round(c(rmse(exp(fitted(final_modelp)), train_new$SalePrice),
                               rmse(exp(predict(final_modelp, newdata = test_new)), test_new$SalePrice)),2))
results
```

## Interpret the Model
The Residuals vs Fitted plot shows that the residuals follow a good linear pattern, which meets the regression assumptions very well. The Normal Q-Q plot shows that the standard errors are mostly normally distributed with a small amount of deviating prediction at the
upper end of the tail and the beginning of the tail. The scale-location plot shows that homoscedasticity is satisfied as a nearly straight line is obtained. 
The residuals vs leverage plot shows that our model includes some high-leverage (so highly influential in our model) points that deviate significantly (more than 4 standardized residuals away) and asymmetrically from the prediction. An influence plot further confirms this believe.
```{r}
summary(final_model)
par(mfrow=c(2,2))
plot(final_model)

coefficients <- coef(final_model)
coefficients
```

We can see the coefficient with largest value (in absolute value) is log(X1stFlrSF) with positive impact, 0.385. Meaning that with all other things being equal(c.p) when the first floor square feet increases, the log-price also increases. Also, the level "BsmtQualNo Basement" has negative impact (-0.227). Meaning that the houses that have no basement will have a decrease on the sales price (c.p). 