#plot(m1)
anova(m0,m1) # rss
as.matrix(AIC(m0, m1))
display(m2 <- lm(log(SalePrice) ~ OverallQual*log(GrLivArea), data = train_new)) # with interaction
display(m2 <- lm(log(SalePrice) ~ OverallQual+log(GrLivArea), data = train_new)) # without interaction
#plot(m2)
anova(m1,m2)
as.matrix(AIC(m1, m2))
display(m3 <- lm(log(SalePrice) ~OverallQual+log(GrLivArea)*GarageCars , data = train_new))
display(m3 <- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars , data = train_new))
anova(m2,m3)
as.matrix(AIC(m2, m3))
summary(m3)
display(m4 <- lm(log(SalePrice)~OverallQual+log(GrLivArea)+GarageCars+GarageArea , data = train_new))
anova(m3,m4)
as.matrix(AIC(m3, m4))
summary(m4)
display(m5<- lm(log(SalePrice) ~OverallQual +log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF , data = train_new))
anova(m4,m5)
summary(m5)
as.matrix(AIC(m4, m5))
display(m6<- lm(log(SalePrice) ~OverallQual +log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+ X1stFlrSF, data = train_new))
anova(m5,m6)
summary(m6)
as.matrix(AIC(m5, m6))  #does not improve
display(m7<- lm(log(SalePrice) ~OverallQual +log(GrLivArea)+GarageCars+TotalBsmtSF+FullBath, data = train_new))
anova(m5,m7)  # does not improve much
summary(m7)
as.matrix(AIC(m5, m7))
display(m8<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+TotRmsAbvGrd, data = train_new))
anova(m5,m8) # not improve
as.matrix(AIC(m7, m8))
display(m9<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt, data = train_new))
anova(m5,m9)
summary(m8)
as.matrix(AIC(m8, m9))  # improves
display(m10<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd, data = train_new))
anova(m9,m10)
summary(m10)
as.matrix(AIC(m9, m10))
display(m11<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+MasVnrArea, data = train_new))
anova(m10,m11)
summary(m11)
as.matrix(AIC(m10, m11))  # not much better
display(m12<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces, data = train_new))
anova(m10,m12)
summary(m12) # r^2 better
as.matrix(AIC(m10, m12))
display(m13<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1, data = train_new))
anova(m12,m13)
summary(m13) # r^2 NOT better , rss and AIC yes -> keep
as.matrix(AIC(m10, m13))
display(m14<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1+LotFrontage, data = train_new))
anova(m13,m14) # R^2 bbetter -> keep
summary(m14)
as.matrix(AIC(m13, m14))
display(m15<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1+LotFrontage+WoodDeckSF, data = train_new))
anova(m14,m15)
summary(m15)
as.matrix(AIC(m14, m15))
# delete garagearea bv very correlated to garage
display(m15<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1+LotFrontage+WoodDeckSF, data = train_new))
anova(m14,m15)
summary(m15)
as.matrix(AIC(m14, m15))
mod.fow <- stats::step(lm(SalePrice~. , data = train_new), trace = FALSE,
direction = "forward")
summary(mod.fow)
# Models:
n00<- lm(log(SalePrice) ~ LotArea + OverallQual + OverallCond + YearBuilt +
MasVnrArea + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF +
X1stFlrSF + X2ndFlrSF  +
BsmtFullBath + FullBath + HalfBath + BedroomAbvGr +
KitchenAbvGr + Fireplaces + GarageCars  +
OpenPorchSF + X3SsnPorch + ScreenPorch +
PoolArea, data = train_new)
#summary(n00)
summary(n00)$r.squared
par(mfrow=c(2,2))
plot(n00)
par(mfrow=c(1,1))
influencePlot(n00)
corrplot(descrCor, method = 'color')
# check if there are any correlated predictors
treshold_vif<-function(rsq){
vif<- 1/(1-rsq)
return(vif)
}
treshold_vif(summary(n00)$r.squared)
vif(n00) # there are not
which(as.vector(vif(n00))>treshold_vif(summary(n00)$r.squared))# there are not highly correlated variables in the model
# delete with a backwards stepwise : MasVnrArea, FullBath, HalfBath, BedroomAbvGr,OpenPorchSF, X3SsnPorch
n0<- lm(log(SalePrice) ~  LotArea + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea, data = train_new)
# Check BoxTidwell tests:
# We guess that the variables with a higher range will have a logaritmic transformation (the area variables)
n0<- lm(log(SalePrice) ~  LotArea + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea, data = train_new)
# we do BoxTidwall test to see which transformations we shall apply in the predictors to make a better model.
# LotArea
boxTidwell(log(SalePrice) ~ LotArea, ~OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea,data=train_new, max.iter = 70 )
v0<- lm(log(SalePrice) ~  log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea, data = train_new)
# we shall convert the LotArea variable into logaritmic
# X1stFlrSF
boxTidwell(log(SalePrice) ~ X1stFlrSF,~ log(LotArea)+ OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+ X2ndFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea,data=train_new, max.iter = 70 )
v1<- lm(log(SalePrice) ~  log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+ log(X1stFlrSF)+ X2ndFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea, data = train_new)
# X2stFlrSF
boxTidwell(log(SalePrice) ~ I(X2ndFlrSF+0.01),~ log(LotArea)+ OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+X1stFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea,data=train_new, max.iter = 70 ) # we dont need
#summary(v1)
summary(v1)$r.squared
par(mfrow=c(2,2))
plot(v1)
par(mfrow=c(1,1))
#influencePlot(v1)
which(as.vector(vif(v1))>treshold_vif(summary(v1)$r.squared))# there are not highly correlated variables in the model
mod.fow <- stats::step(lm(SalePrice~. , data = train_new), trace = FALSE,
direction = "forward")
summary(mod.fow)
# Models:
n00<- lm(log(SalePrice) ~ LotArea + OverallQual + OverallCond + YearBuilt +
MasVnrArea + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF +
X1stFlrSF + X2ndFlrSF  +
BsmtFullBath + FullBath + HalfBath + BedroomAbvGr +
KitchenAbvGr + Fireplaces + GarageCars  +
OpenPorchSF + X3SsnPorch + ScreenPorch +
PoolArea, data = train_new)
#summary(n00)
summary(n00)$r.squared
par(mfrow=c(2,2))
plot(n00)
par(mfrow=c(1,1))
influencePlot(n00)
corrplot(descrCor, method = 'color')
# check if there are any correlated predictors
treshold_vif<-function(rsq){
vif<- 1/(1-rsq)
return(vif)
}
treshold_vif(summary(n00)$r.squared)
vif(n00) # there are not
which(as.vector(vif(n00))>treshold_vif(summary(n00)$r.squared))# there are not highly correlated variables in the model
# delete with a backwards stepwise : MasVnrArea, FullBath, HalfBath, BedroomAbvGr,OpenPorchSF, X3SsnPorch
n0<- lm(log(SalePrice) ~  LotArea + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea, data = train_new)
# Check BoxTidwell tests:
# We guess that the variables with a higher range will have a logaritmic transformation (the area variables)
n0<- lm(log(SalePrice) ~  LotArea + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea, data = train_new)
# we do BoxTidwall test to see which transformations we shall apply in the predictors to make a better model.
# LotArea
boxTidwell(log(SalePrice) ~ LotArea, ~OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea,data=train_new, max.iter = 70 )
v0<- lm(log(SalePrice) ~  log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea, data = train_new)
# we shall convert the LotArea variable into logaritmic
# X1stFlrSF
boxTidwell(log(SalePrice) ~ X1stFlrSF,~ log(LotArea)+ OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+ X2ndFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea,data=train_new, max.iter = 70 )
v1<- lm(log(SalePrice) ~  log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+ log(X1stFlrSF)+ X2ndFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea, data = train_new)
# X2stFlrSF
boxTidwell(log(SalePrice) ~ I(X2ndFlrSF+0.01),~ log(LotArea)+ OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + BsmtFinSF2+X1stFlrSF +
BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea,data=train_new, max.iter = 70 ) # we dont need
#summary(v1)
summary(v1)$r.squared
par(mfrow=c(2,2))
plot(v1)
par(mfrow=c(1,1))
#influencePlot(v1)
which(as.vector(vif(v1))>treshold_vif(summary(v1)$r.squared))# there are not highly correlated variables in the model
rmse <- function(fitted, actual){
sqrt(mean((fitted - actual)^2))
}
RSQUARE <- function(predictions, actual_values) {
ss_residual <- sum((actual_values - predictions)^2)
ss_total <- sum((actual_values - mean(actual_values))^2)
rsquare <- 1 - (ss_residual / ss_total)
return(rsquare)
}
results <- data.frame(Model = c("Model with train_new sample",
"Model with test_new sample"),
RSQRT = round(c(RSQUARE(exp(fitted(v1)), train_new$SalePrice),
RSQUARE(exp(predict(v1, newdata = test_new)), test_new$SalePrice)),2),
RMSE = round(c(rmse(exp(fitted(v1)), train_new$SalePrice),
rmse(exp(predict(v1, newdata = test_new)), test_new$SalePrice)),2))
results
#names(train_new)
b0<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea + Neighborhood
, data = train_new)
#summary(b0)
summary(b0)$r.squared
anova(v1,b0)
AIC(v1,b0)
treshold_vif(summary(b0)$r.squared)
vif(b0) # High vif-> Highly correlated predictor -> wont add to the model
b1<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual
, data = train_new)
#summary(b1)
summary(b1)$r.squared
anova(n0,b1)
AIC(n0,b1)
treshold_vif(summary(b1)$r.squared)
vif(b1)
# check interactions
b11<- lm(log(SalePrice) ~ (LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea)*ExterQual
, data = train_new)
#summary(b11)
summary(b11)$r.squared
anova(b1,b11) # the model is better with interactions but for explainability reasons we will not consider the interactions
AIC(b1,b11)
b2<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual + BsmtQual
, data = train_new)
#summary(b2)
summary(b2)$r.squared
anova(b1,b2)
AIC(b1,b2)
treshold_vif(summary(b2)$r.squared)
vif(b2)
# interactions
b21<- lm(log(SalePrice) ~ (LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual) *BsmtQual
, data = train_new)
#summary(b21)
summary(b21)$r.squared
anova(b2,b21) #  better with interaction
AIC(b2,b21)
anova(b11,b21) # but worse than b11
b3<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual
, data = train_new)
#summary(b3)
summary(b3)$r.squared
anova(b2,b3)
AIC(b2,b3)
treshold_vif(summary(b3)$r.squared)
vif(b3)
#with interactions
b31<- lm(log(SalePrice) ~ (LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual + BsmtQual)* KitchenQual
, data = train_new)
#summary(b31)
summary(b31)$r.squared
anova(b3,b31) #  better model with interactions but we will not consider them for explainability pourposes
AIC(b3,b31)
b4<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+ GarageFinish
, data = train_new)
#summary(b4)
summary(b4)$r.squared
anova(b3,b4) # not better model -> we dont use this variable
AIC(b3,b4)
treshold_vif(summary(b4)$r.squared)
vif(b4)
b5<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual + FireplaceQu
, data = train_new)
#summary(b5)
summary(b5)$r.squared
anova(b3,b5) # not better model
AIC(b3,b5)
treshold_vif(summary(b5)$r.squared)
vif(b5)
b6<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+Foundation
, data = train_new)
#summary(b6)
summary(b6)$r.squared
anova(b3,b6)
AIC(b3,b6)
treshold_vif(summary(b6)$r.squared)
vif(b6) # we dont add it because highly correlated
b7<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType
, data = train_new)
#summary(b7)
summary(b7)$r.squared
anova(b3,b7)
AIC(b3,b7)
treshold_vif(summary(b7)$r.squared)
vif(b7)
# interactions
b71<- lm(log(SalePrice) ~ (LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea)*GarageType +ExterQual + BsmtQual + KitchenQual
, data = train_new)
#summary(b71)
summary(b71)$r.squared
anova(b7,b71) # better model
AIC(b7,b71)
treshold_vif(summary(b71)$r.squared)
b8<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType+MSSubClass
, data = train_new)
#summary(b8)
summary(b8)$r.squared
anova(b7,b8) # not better model
AIC(b7,b8)
treshold_vif(summary(b8)$r.squared)
vif(b8) # highly correlated
b9<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType+BsmtFinType1
, data = train_new)
#summary(b9)
summary(b9)$r.squared
anova(b7,b9)  # not better model
AIC(b7,b9)
treshold_vif(summary(b9)$r.squared)
#f.LotArea
b10<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType+f.LotArea
, data = train_new)
#summary(b10)
summary(b10)$r.squared
anova(b7,b10) # not better model
AIC(b7,b10)
treshold_vif(summary(b10)$r.squared)
vif(b10)
b12<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType+HasPorch_binary
, data = train_new)
#summary(b11)
summary(b11)$r.squared
anova(b7,b11) # a little better model, but not much, we better keep simpler model
AIC(b7,b11)
AIC(b7,b71,b3,b31,b2,b21,b1,b11)
# now we have done a forward stepwise, we can see if we need all the variables doing a backwards stepwise, we can see that in the model: LotFrontage PoolArea and GarageType are less influental, for better explainability we will not consider them in our model.
final_model<-b3
AIC(b7,b71,b3,b31,b2,b21,b1,b11)
final_model<-b1
results <- data.frame(Model = c("Model with train_new sample",
"Model with test_new sample"),
RSQRT = round(c(RSQUARE(exp(fitted(final_model)), train_new$SalePrice),
RSQUARE(exp(predict(final_model, newdata = test_new)), test_new$SalePrice)),2),
RMSE = round(c(rmse(exp(fitted(final_model)), train_new$SalePrice),
rmse(exp(predict(final_model, newdata = test_new)), test_new$SalePrice)),2))
results
mean_hat <- mean(hatvalues(final_model));mean_hat
priori <- which(hatvalues(final_model)>4*mean_hat)
length(priori)
betas <- as.data.frame(dfbetas(final_model))
betas_cutoff <- 2 / sqrt(dim(train_new)[1]);betas_cutoff
influencePlot(final_model, id=list(n=3, method="noteworthy"))
summary(final_model)
plot(final_model)
coefficients <- coef(final_model)
coefficients
AIC(b7,b71,b3,b31,b2,b21,b1,b11)
final_model<-b1
results <- data.frame(Model = c("Model with train_new sample",
"Model with test_new sample"),
RSQRT = round(c(RSQUARE(exp(fitted(final_model)), train_new$SalePrice),
RSQUARE(exp(predict(final_model, newdata = test_new)), test_new$SalePrice)),2),
RMSE = round(c(rmse(exp(fitted(final_model)), train_new$SalePrice),
rmse(exp(predict(final_model, newdata = test_new)), test_new$SalePrice)),2))
results
final_model<-b3
results <- data.frame(Model = c("Model with train_new sample",
"Model with test_new sample"),
RSQRT = round(c(RSQUARE(exp(fitted(final_model)), train_new$SalePrice),
RSQUARE(exp(predict(final_model, newdata = test_new)), test_new$SalePrice)),2),
RMSE = round(c(rmse(exp(fitted(final_model)), train_new$SalePrice),
rmse(exp(predict(final_model, newdata = test_new)), test_new$SalePrice)),2))
results
final_model<-b3
results <- data.frame(Model = c("Model with train_new sample",
"Model with test_new sample"),
RSQRT = round(c(RSQUARE(exp(fitted(final_model)), train_new$SalePrice),
RSQUARE(exp(predict(final_model, newdata = test_new)), test_new$SalePrice)),2),
RMSE = round(c(rmse(exp(fitted(final_model)), train_new$SalePrice),
rmse(exp(predict(final_model, newdata = test_new)), test_new$SalePrice)),2))
results
mean_hat <- mean(hatvalues(final_model));mean_hat
priori <- which(hatvalues(final_model)>4*mean_hat)
length(priori)
betas <- as.data.frame(dfbetas(final_model))
betas_cutoff <- 2 / sqrt(dim(train_new)[1]);betas_cutoff
influencePlot(final_model, id=list(n=3, method="noteworthy"))
llcoo <-c("1424","811","1183")
# residual outliers
llres <- which(abs(rstudent(final_model))>qnorm(0.995));length(llres)
llrem <- unique(c(rownames(train_new)[llres],llcoo)); length(llrem)
llremreg<-which(rownames(train_new)%in%llrem);llremreg
train_new<-train_new[-llremreg,]
final_modelp <- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
BsmtFullBath + FullBath   + TotRmsAbvGrd+
KitchenAbvGr + Fireplaces + GarageCars  +
ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType
, data = train_new)
summary(final_modelp)
final_modelp <- step( final_modelp, k=log(nrow(train_new)))
par(mfrow=c(2,2))
plot(final_modelp)
coef_orig <- coef(final_model)
coef_filt <- coef(final_modelp)
#common variables
var_com <- intersect(names(coef_orig), names(coef_filt))
coef_orig <- coef_orig[var_com]
coef_filt <- coef_filt[var_com]
coef_comparison <- data.frame(Variable = names(coef_orig),
Original_coefficient = coef_orig,
Filtered_coefficient = coef_filt)
print(coef_comparison)
results <- data.frame(Model = c("Model with train_new sample",
"Model with test_new sample"),
RSQRT = round(c(RSQUARE(exp(fitted(final_modelp)), train_new$SalePrice),
RSQUARE(exp(predict(final_modelp, newdata = test_new)), test_new$SalePrice)),2),
RMSE = round(c(rmse(exp(fitted(final_modelp)), train_new$SalePrice),
rmse(exp(predict(final_modelp, newdata = test_new)), test_new$SalePrice)),2))
results
summary(final_model)
par(mfrow=c(2,2))
plot(final_model)
coefficients <- coef(final_model)
coefficients
#setwd("C:/Users/Silvia/OneDrive - Universitat Politècnica de Catalunya/Escritorio/UPC/MASTER DS/1A/SIM/SIM_assignment_1")
setwd("/Users/ali/Desktop/MASTER/SIM/PROJECT 1")
train<-read.delim("train.csv", sep=',')
shapiro.test(log(train$SalePrice))
shapiro.test(train$SalePrice)
shapiro.test(log(train$SalePrice))
qqnorm(train$SalePrice)
hist(train$SalePrice, prob = TRUE, col = 'lightblue', main = 'SalePrice Distribution', xlab = 'SalePrice')
lines(density(train$SalePrice), col = 'red', lwd = 2)
summary(train2$BsmtFullBath)
mod.fow <- stats::step(lm(SalePrice~. , data = train_new), trace = FALSE,
direction = "forward")
numeric_description(train2$OpenPorchSF, 30)
