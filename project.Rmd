---
title: "Assignment 1"
author: "Alicia Chimeno & Silvia Ferrer"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Clear space and Libraries
```{r, include=FALSE}
library(tidyverse)
library(skimr)
library(FactoMineR)
library(car)
library(lmtest)
library(ISLR)
library(arm)
library(mice)
library(corrplot)
library(chemometrics)
library(dplyr)
library(caret)
library(FactoMineR)
library(corrplot)
rm(list=ls())
par(mfrow=c(1,1))
```
# Load dataset
```{r, include=FALSE}
#setwd("C:/Users/Silvia/OneDrive - Universitat Politècnica de Catalunya/Escritorio/UPC/MASTER DS/1A/SIM/SIM_assignment_1")
setwd("/Users/ali/Desktop/MASTER/SIM/PROJECT 1")
train<-read.delim("train.csv", sep=',') 
```

# Explore and Preproces the dataset
We start by looking at an overview of our dataset.
```{r}
dim(train)
str(train)
```

## Preprocess and select variables
### Record incorrect NA's
```{r}
train$Alley[which(is.na(train$Alley))] <- "No alley access"
train$BsmtQual[which(is.na(train$BsmtQual))] <- "No Basement"
train$BsmtCond[which(is.na(train$BsmtCond))] <- "No Basement"
train$BsmtExposure[which(is.na(train$BsmtExposure))] <- "No Basement"
train$BsmtFinType1[which(is.na(train$BsmtFinType1))] <- "No Basement"
train$BsmtFinType2[which(is.na(train$BsmtFinType2))] <- "No Basement"
train$FireplaceQu[which(is.na(train$FireplaceQu))] <- "No Fireplace"
train$GarageType[which(is.na(train$GarageType))] <- "No Garage"
train$GarageFinish[which(is.na(train$GarageFinish))] <- "No Garage"
train$GarageCond[which(is.na(train$GarageCond))] <- "No Garage"
train$PoolQC[which(is.na(train$PoolQC))] <- "No Pool"
train$Fence[which(is.na(train$Fence))] <- "No Fence"
train$MiscFeature[which(is.na(train$MiscFeature))] <- "None"
```

## Recode variables to factor

```{r}
# Recode: character to factor
char_cols <- which(sapply(train, is.character))
train[, char_cols] <- lapply(train[, char_cols], as.factor) 

# Recode: numeric to factor
train$MSSubClass <- factor(train$MSSubClass)
train$MoSold <- factor(train$MoSold) # month 
```

## Target variable exploration "SalePrice"
We asses a normality test to the target variable. We reject null hypothesis (H0: variable is normal), therefore we shall not consider that the target variable is normally distributed.
```{r}
res.con <- condes(train,81)
head(res.con$quali)
shapiro.test(train$SalePrice) 
par(mfrow=c(1,2))
shapiro.test(log(train$SalePrice)) # log-normality
qqnorm(train$SalePrice)
qqline(train$SalePrice)
hist(train$SalePrice, prob = TRUE, col = 'lightblue', main = 'SalePrice Distribution', xlab = 'SalePrice')
lines(density(train$SalePrice), col = 'red', lwd = 2)
```

## Selection of 10 factor variables and numerical variables 
We select the first 10 categorical variables that are more associated with the target variable.
```{r}
vect<-row.names(res.con$quali)
vect[1:10] 
factor_train<-train[,c(vect[1:10])]
num_cols <- names(train)[sapply(train, is.numeric)]
train2 <- train[,c(num_cols, vect[1:10])]
```

# Variable analysis (EDA and univariate outliers)
## Analysis functions
To proceed with the predictor analysis, we do create the following functions for analyzing our numerical features:
```{r}
numeric_description <- function(variable, n_breaks) {
  #cat("Summary:\n")
  #print(summary(variable))
  cat("Count of missing values:",sum(is.na(variable)),"\n")
  column_name <- sub(".+\\$", "", deparse(substitute(variable)))
  hist(variable, breaks = n_breaks, freq = F, main = paste("Histogram of", column_name), xlab = column_name)
  curve(dnorm(x, mean(variable), sd(variable)), add = T)
  print(shapiro.test(variable))
}

analyze_outliers <- function(data, column_name) {
  sm <- summary(data[[column_name]])
  iqr <- sm["3rd Qu."] - sm["1st Qu."]
  # Mild Outliers
  mild_ub <- sm["3rd Qu."] + 1.5 * iqr
  mild_lb <- sm["1st Qu."] - 1.5 * iqr
  mild_outliers <- length(which(data[[column_name]] > mild_ub | data[[column_name]] < mild_lb))
  cat("Number of mild outliers:", mild_outliers, "\n")
  Boxplot(data[[column_name]], main = paste("Outlier Analysis for", column_name),
          ylab = column_name, outline = TRUE)
  abline(h = mild_ub, col = "orange", lwd = 2)
  abline(h = mild_lb, col = "orange", lwd = 2)
  # Severe Outliers
  severe_ub <- sm["3rd Qu."] + 3 * iqr
  severe_lb <- sm["1st Qu."] - 3 * iqr
  severe_outliers <- length(which(data[[column_name]] > severe_ub | data[[column_name]] < severe_lb))
  cat("Number of severe outliers:", severe_outliers, "\n")
  abline(h = severe_ub, col = "red", lwd = 2.5)
  abline(h = severe_lb, col = "red", lwd = 2.5)
}

train2$univ_outl_count <- 0 # new column count outliers - initialize

update_outliers_count <- function(dataframe, column_name) {
  df <- dataframe
  sm <- summary(df[[column_name]])
  iqr <- sm["3rd Qu."] - sm["1st Qu."]
  severe_ub <- sm["3rd Qu."] + 3 * iqr
  severe_lb <- sm["1st Qu."] - 3 * iqr
  severe_outliers_id <- which(df[[column_name]] > severe_ub | df[[column_name]] < severe_lb)
  df$univ_outl_count[severe_outliers_id] <- df$univ_outl_count[severe_outliers_id] + 1
  return(df)
}
```

We continue performing a descriptive analysis of each variable. Therefore, we are using the function update_outliers_count where we are updating a new variable named univ_outl_count. It is a count of the total univariate outliers an instance has, in order to track the data quality and associate having univariate outliers to some features with a correlation matrix.
The order of this descriptive analysis follows the criteria of splitting into numerical and then categorical variables.

```{r, results='hide'}
skim(train2) # overall exploration
summary(train2)
#names(train2)
```

## Numerical variables
Firstly, we are analyzing the numerical variables, excluding the variable 'Id', which is not considered for the analysis. We distinguish between continuous variables (areas,) and discrete variables (number of garages, rooms...) 

### "LotFrontage" Linear feet of street connected to property (cont)
```{r}
par(mfrow=c(1,2))
numeric_description(train2$LotFrontage, 30) # not normal distribution
analyze_outliers(train2, "LotFrontage")
train2 <- update_outliers_count(train2, "LotFrontage")
```


### "LotArea" Lot size in square feet (cont)

```{r}
par(mfrow=c(1,3))
numeric_description(train2$LotArea, 20) # not normal distribution
analyze_outliers(train2, "LotArea")
train2 <- update_outliers_count(train2, "LotArea")

# new variable:
sm<-summary(train2$LotArea)
plot(train2$LotArea,train2$SalePrice)

iqr <- sm["3rd Qu."] - sm["1st Qu."]
mild_ub <- sm["3rd Qu."] + 1.5 * iqr
mild_lb <- sm["1st Qu."] - 1.5 * iqr

train2$f.LotArea <- ifelse(train2$LotArea <= mild_lb, 1, 
                    ifelse(train2$LotArea > mild_lb & train2$LotArea < mild_ub, 2,
                    ifelse(train2$LotArea >= mild_ub, 3, NA)))

train2$f.LotArea<- factor(train2$f.LotArea, labels=c("LowLotArea","MidLotArea","HighLotArea"), order = T, levels=c(1,2,3))
#table(train2$f.LotArea)
```

### "OverallQual" (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$OverallQual, 10) # not normal distribution
analyze_outliers(train2, "OverallQual")
train2 <- update_outliers_count(train2, "OverallQual")
```

### "YearBuilt" (int)
```{r}
summary(train2$YearBuilt)
par(mfrow=c(1,2))
barplot(table(train2$YearBuilt), main = "Distribution of Year Built",xlab = "Year",col = "skyblue")
sum(is.na(train2$YearBuilt))
analyze_outliers(train2, "YearBuilt")
train2 <- update_outliers_count(train2, "YearBuilt")
```

### "YearRemodAdd" (int) - Remodel date (same as construction date if no remodeling or additions)
```{r}
summary(train2$YearRemodAdd)
barplot(table(train2$YearRemodAdd), main = "Distribution of Year Remodelized",xlab = "Year",col = "skyblue")
sum(is.na(train2$YearRemodAdd))
analyze_outliers(train2, "YearRemodAdd")
train2 <- update_outliers_count(train2, "YearRemodAdd")
```


### MasVnrArea - Masonry veneer area in square feet (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$MasVnrArea, 10)# not normal distribution
analyze_outliers(train2,"MasVnrArea")
train2 <- update_outliers_count(train2, "MasVnrArea")
# find values that are NA instead of 0
# if it has a material but a 0 in masvnrarea is a NA
train2$MasVnrArea[which(train2$MasVnrArea == 0)] <- NA
none_idx <- which(train$MasVnrType == 'None')
train2$MasVnrArea[none_idx] <- 0
```


### "BsmtFinSF1" - Type 1 finished square feet (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$BsmtFinSF1, 10)# not normal distribution
analyze_outliers(train2,"BsmtFinSF1")
train2 <- update_outliers_count(train2, "BsmtFinSF1")
```

### BsmtFinSF2 - Rating of basement finished area (if multiple types) (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$BsmtFinSF2, 10) # not normal distribution
analyze_outliers(train2,"BsmtFinSF2")
train2 <- update_outliers_count(train2, "BsmtFinSF2")
```

### "BsmtUnfSF" (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$BsmtUnfSF, 20) # not normal distribution
analyze_outliers(train2,"BsmtUnfSF")
train2 <- update_outliers_count(train2, "BsmtUnfSF")
```

### "TotalBsmtSF" (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$TotalBsmtSF, 10)# not normal distribution
analyze_outliers(train2,"TotalBsmtSF")
train2 <- update_outliers_count(train2, "TotalBsmtSF")
```

### X1stFlrSF - First Floor square feet (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$X1stFlrSF, 10) # not normal distribution
analyze_outliers(train2,"X1stFlrSF")
train2 <- update_outliers_count(train2, "X1stFlrSF")
```

### X2ndFlrSF - Second floor square feet (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$X2ndFlrSF, 10)# not normal distribution
analyze_outliers(train2,"X2ndFlrSF")
train2 <- update_outliers_count(train2, "X2ndFlrSF")

# create a new variable if has second floor = 1 / no second floor = 0. 
train2$secondfloor<-0
train2$secondfloor[which(train2$X2ndFlrSF!=0)]<-1
train2$secondfloor<-factor(train2$secondfloor)
table(train2$secondfloor) 
```

### LowQualFinSF - Low quality finished square feet (all floors) (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$LowQualFinSF, 10) # not normal distribution
#table(train2$LowQualFinSF)# very centered in 0 
analyze_outliers(train2,"LowQualFinSF")
nearZeroVar(train2$LowQualFinSF, saveMetrics = TRUE) # Variable with near zero variance
train2 <- update_outliers_count(train2, "LowQualFinSF")
```

### GrLivArea - Above grade (ground) living area square feet (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$GrLivArea, 20) # not normal distribution
analyze_outliers(train2,"GrLivArea")
train2 <- update_outliers_count(train2, "GrLivArea")
```

### BsmtFullBath - # full baths in the basement (int)

```{r}
summary(train2$BsmtFullBath) 
par(mfrow=c(1,2))
analyze_outliers(train2,"BsmtFullBath")
train2 <- update_outliers_count(train2, "BsmtFullBath")
#table(train2$BsmtFullBath)
sum(is.na(train2$BsmtFullBath))
barplot(table(train2$BsmtFullBath), main = "Distribution of Baths in the basement",xlab = "Number of Baths",col = "skyblue")
```

### BsmtHalfBath - # half baths in the basement (int)

```{r}
summary(train2$BsmtHalfBath) 
analyze_outliers(train2,"BsmtHalfBath")
train2 <- update_outliers_count(train2, "BsmtHalfBath")
table(train2$BsmtHalfBath) # very centered in 0
nearZeroVar(train2$BsmtHalfBath, saveMetrics = TRUE) # Variable not with near zero variance
sum(is.na(train2$BsmtHalfBath))
barplot(table(train2$BsmtHalfBath), main = "Distribution of Half Baths in the basement",xlab = "Number of Baths",col = "skyblue")

```

### FullBath - full baths above grade (int)

```{r}
summary(train2$FullBath) 
par(mfrow=c(1,2))
analyze_outliers(train2,"FullBath")
train2 <- update_outliers_count(train2, "FullBath")
table(train2$FullBath)
sum(is.na(train2$FullBath))
barplot(table(train2$FullBath), main = "Distribution of Baths above grade",xlab = "Number of Baths",col = "skyblue")
```

### HalfBath - Half baths above grade (int)

```{r}
summary(train2$HalfBath) 
par(mfrow=c(1,2))
analyze_outliers(train2,"HalfBath")
train2 <- update_outliers_count(train2, "HalfBath")
table(train2$HalfBath)
sum(is.na(train2$HalfBath))
barplot(table(train2$HalfBath), main = "Distribution of Half Baths above grade",xlab = "Number of Baths",col = "skyblue")
```

### BedroomAbvGr - Bedrooms above grade (does NOT include basement bedrooms)

```{r}
summary(train2$BedroomAbvGr) 
par(mfrow=c(1,2))
analyze_outliers(train2,"BedroomAbvGr")
train2 <- update_outliers_count(train2, "BedroomAbvGr")
table(train2$BedroomAbvGr)
sum(is.na(train2$BedroomAbvGr))
barplot(table(train2$BedroomAbvGr), main = "Distribution of Bedrooms above grade",xlab = "Number of Bedrooms",col = "skyblue")
```

### KitchenAbvGr - Kitchens above grade (int)

```{r}
summary(train2$KitchenAbvGr) 
par(mfrow=c(1,2))
analyze_outliers(train2,"KitchenAbvGr")
train2 <- update_outliers_count(train2, "KitchenAbvGr")
sum(is.na(train2$KitchenAbvGr))
barplot(table(train2$KitchenAbvGr), main = "Distribution of Kitchens",xlab = "Number of Kitchens",col = "skyblue")
table(train2$KitchenAbvGr) # variable very centered in 1
nearZeroVar(train2$KitchenAbvGr,saveMetrics = TRUE) # near zero variance
```

### "TotRmsAbvGrd" - Total rooms above grade (does not include bathrooms) - continuous ratio variable (int)

```{r}
par(mfrow=c(1,2))
summary(train2$TotRmsAbvGrd)
analyze_outliers(train2,"TotRmsAbvGrd")
train2 <- update_outliers_count(train2, "TotRmsAbvGrd")
#table(train2$TotRmsAbvGrd)
sum(is.na(train2$TotRmsAbvGrd))

```

### "Fireplaces" - Number of fireplaces (int)

```{r}
summary(train2$Fireplaces)
par(mfrow=c(1,2))
#table(train2$Fireplaces)
sum(is.na(train2$Fireplaces))
analyze_outliers(train2, "Fireplaces")
train2 <- update_outliers_count(train2, "Fireplaces")
barplot(table(train2$Fireplaces), main = "Distribution of fireplaces",xlab = "Number of Fireplaces",col = "skyblue")
```

### "GarageCars" - Size of garage in car capacity (int)

```{r}
summary(train2$GarageCars)
#table(train2$GarageCars)
sum(is.na(train2$GarageCars)) #0
par(mfrow=c(1,2))
analyze_outliers(train2, "GarageCars") 
train2 <- update_outliers_count(train2, "GarageCars")
barplot(table(train2$GarageCars), main = "Distribution of Garage Cars",xlab = "Number of Garage Cars",col = "skyblue")
```

### "GarageArea" - Size of garage in square feet (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$GarageArea, 30) # not normal distribution
analyze_outliers(train2, "GarageArea")
train2 <- update_outliers_count(train2, "GarageArea")
sm<-summary(train2$GarageArea)

# checking if 0 mean that they don't have garage
which(train$GarageCars == 0)
which(train$GarageArea == 0)
which(train$GarageFinish=='No Garage')
```

### "WoodDeckSF" - Wood deck area in square feet - continuous ratio variable

```{r}
par(mfrow=c(1,2))
numeric_description(train2$WoodDeckSF, 30) # not normal distribution
#table(train2$WoodDeckSF) # centered data in 0
analyze_outliers(train2, "WoodDeckSF")
train2 <- update_outliers_count(train2, "WoodDeckSF")
```

### "OpenPorchSF" - Open porch area in square feet - continuous ratio variable (continua)

```{r}
numeric_description(train2$OpenPorchSF, 30)
par(mfrow=c(1,2))
analyze_outliers(train2,"OpenPorchSF")
train2 <- update_outliers_count(train2, "OpenPorchSF")
#table(train2$OpenPorchSF) # very centered data in 0
train2$OpenPorch_binary<-0
train2$OpenPorch_binary[which(train2$OpenPorchSF!=0)]<-1
train2$OpenPorch_binary<-as.factor(train2$OpenPorch_binary)
table(train2$OpenPorch_binary)

```

### EnclosedPorch - Enclosed porch area in square feet (cont)

```{r}
numeric_description(train2$EnclosedPorch, 30)
par(mfrow=c(1,2))
analyze_outliers(train2,"EnclosedPorch")
train2 <- update_outliers_count(train2, "EnclosedPorch")
#table(train2$EnclosedPorch) # very centered data in 0

# therefore we will create a factor 0 if they dont have enclosed porch area,1 if they do have.
train2$EnclosedPorch_binary<-0
train2$EnclosedPorch_binary[which(train2$EnclosedPorch!=0)]<-1
train2$EnclosedPorch_binary<-as.factor(train2$EnclosedPorch_binary)
#table(train2$EnclosedPorch_binary)

# create new variable - has porch or not - mixing both binary variables
train2$HasPorch_binary<-0
train2$HasPorch_binary[which(train2$EnclosedPorch!=0 | train2$OpenPorchSF!=0)]<-1
train2$HasPorch_binary<-as.factor(train2$HasPorch_binary)
table(train2$HasPorch_binary)
```

### X3SsnPorch - Three season porch area in square feet (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$X3SsnPorch, 30)
#table(train2$X3SsnPorch) # very centered in 0 -> dont have three season porch
analyze_outliers(train2,"X3SsnPorch")
train2 <- update_outliers_count(train2, "X3SsnPorch")

```

### ScreenPorch - Screen porch area in square feet (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$ScreenPorch, 30) # not normally distributed
#table(train2$ScreenPorch) # very centered in 0 -> dont have screen porch
analyze_outliers(train2,"ScreenPorch")
train2 <- update_outliers_count(train2, "ScreenPorch")
```

### PoolArea - Pool area in square feet (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$PoolArea, 30)# not normally distributed
#table(train2$PoolArea) # very centered in 0 -> dont have pool
analyze_outliers(train2,"PoolArea")
train2 <- update_outliers_count(train2, "PoolArea")

# create a new variable : 0=no pool/1= pool
train2$Pool_binary<-0
train2$Pool_binary[which(train2$PoolArea!=0)]<-1
train2$Pool_binary<-as.factor(train2$Pool_binary)
table(train2$Pool_binary) # not very usefull , the majority dont have pools, very centered
#train2$Pool_binary<-NULL
```

### MiscVal - $Value of miscellaneous feature (cont)

```{r}
par(mfrow=c(1,2))
numeric_description(train2$MiscVal, 30) # not normally distributed
#table(train2$MiscVal) # very centered in 0 
analyze_outliers(train2,"MiscVal")
train2 <- update_outliers_count(train2, "MiscVal")

```

### YrSold - (int)
```{r}
par(mfrow=c(1,2))
barplot(table(train2$YrSold), main = "Distribution of YrSold",xlab = "Number of Garage Cars",col = "skyblue")
analyze_outliers(train2,"YrSold")
train2 <- update_outliers_count(train2, "YrSold")
```

## Categorical Variables
Categorical variables are analyzed.

### Neighborhood 

```{r, warning=FALSE}
par(mfrow=c(1,2))
sum(is.na(train2$Neighborhood)) #0
#table(train2$Neighborhood)
barplot(table(train2$Neighborhood), main = "Distribution of Neighborhood",xlab = "Number of Neighborhood",col = "skyblue")
pie(table(train2$Neighborhood), main = "Distribution of Neighborhood", col = rainbow(length(levels(train2$Neighborhood))))
```
### ExterQual - Evaluates the quality of the material on the exterior 
```{r}
par(mfrow=c(1,1))
sum(is.na(train2$ExterQual)) #0
#table(train2$ExterQual)
barplot(table(train2$ExterQual), main = "Distribution of the exterior material quality",xlab = "Qualification",col = "skyblue")
```
### BsmtQual - Evaluates the height of the basement
```{r}
sum(is.na(train2$BsmtQual)) #0
#table(train2$BsmtQual)
barplot(table(train2$BsmtQual), main = "Distribution of Basement height Quality",xlab = "Qualification",col = "skyblue")
```
### KitchenQual - Kitchen quality
```{r}
sum(is.na(train2$KitchenQual)) #0
#table(train2$KitchenQual)
barplot(table(train2$KitchenQual), main = "Distribution of Kitchen Quality",xlab = "Qualification",col = "skyblue")
```
### GarageFinish - Interior finish of the garage
```{r}
sum(is.na(train2$GarageFinish)) #0
#table(train2$GarageFinish)
barplot(table(train2$GarageFinish), main = "Garage Finish Distribution",xlab = "Type",col = "skyblue")
```
### FireplaceQu - Fireplace quality
```{r}
sum(is.na(train2$FireplaceQu)) #0
#table(train2$FireplaceQu)
barplot(table(train2$FireplaceQu), main = "Fireplace quality Distribution",xlab = "Qualification",col = "skyblue")
```
### Foundation - Type of foundation
```{r}
sum(is.na(train2$Foundation)) #0
#table(train2$Foundation)
barplot(table(train2$Foundation), main = "Type of foundation Distribution",xlab = "Type",col = "skyblue")
```
### GarageType - Garage location
```{r}
sum(is.na(train2$GarageType)) #0
#table(train2$GarageType)
barplot(table(train2$GarageType), main = "Garage Location Distribution",xlab = "Garage Location",col = "skyblue")
```
### MSSubClass
```{r}
sum(is.na(train2$MSSubClass)) #0
#table(train2$MSSubClass) # +classes
barplot(table(train2$MSSubClass), main = "MSSubClass Distribution",xlab = "MSSubClass",col = "skyblue")
```
## Missing values and imputation
Checking which variables have NA's we can see that LotFrontage, MasVnrArea and GarageYrBlt are the ones with missing values.
```{r}
#skim(train2)
sum(is.na(train2$LotFrontage))
sum(is.na(train2$MasVnrArea))
sum(is.na(train2$GarageYrBlt))
```
We decided to exclude the imputation of the GarageYrBlt feature because it has no sense to impute this variable, as it is a year.
```{r}
mice_imp<-mice(train2[, !names(train2) %in% "GarageYrBlt"],method = "cart")
imputed_data<-complete(mice_imp)
```
We proceed to validate the imputation.
```{r}
# LotFrontage validation
summary(imputed_data$LotFrontage)
summary(train2$LotFrontage)
par(mfrow=c(1,2))
plot(density(train2$LotFrontage,na.rm=TRUE))
plot(density(imputed_data$LotFrontage,na.rm=TRUE))
par(mfrow=c(1,1))
```
Checking the results we can conclude that the imputation doesn't have a significant change in the density nor the summary. 
```{r}
# MasVnrArea validation
summary(imputed_data$MasVnrArea)
summary(train2$MasVnrArea)
par(mfrow=c(1,2))
plot(density(train2$MasVnrArea,na.rm=TRUE))
plot(density(imputed_data$MasVnrArea,na.rm=TRUE))
par(mfrow=c(1,1))
```
# Data Quality Exploration
To check the quality of the dataset we have a count on each individual of the univariate outliers. We can see that there are 7 instances that have 4 or more univariate outliers, therefore are candidates to delete from the analysis. 
```{r}
max(imputed_data$univ_outl_count) 
ind_delete<-imputed_data[which(imputed_data$univ_outl_count > 3),]
imputed_data<-imputed_data[-c(ind_delete)$Id,]
```
## Correlation Matrix
The correlation matrix shows a moderate negative correlation to the BsmtUnfSF (then the less Unfinished square feet of basement area, the more univ outliers) and a significant positive correlation to both BsmtFinSF2, Type 2 finished square feet, and ScreenPorch.
```{r}
cols <- num_cols[! num_cols %in% c("Id","GarageYrBlt")]
df_of_interest <- imputed_data[,c(cols,"univ_outl_count")]
cor_outl = cor(df_of_interest)
par(mfrow=c(1,1))
corrplot(cor_outl, method = 'color')
#cor_outl[34, ]
colnames(cor_outl)[which.min(cor_outl[34, ])];min(cor_outl[34, ])
colnames(cor_outl)[which.max(cor_outl[34,1:33])];max(cor_outl[34,1:33])
```
## Multivariate outliers
We filter variables in order to have only those with similar distribution of values. For that reason, we have filtered the values that had more unique values than 40. Then, we have excluded the variables that were centered in the '0' value.
We are not taking off these outliers because the length is 44 and taking into account the dimensions of our dataset, 1460, it would be a reduction of the 3%.
Also, we have performed other Moutliers in order to see different scenarios that we consider rare data.
```{r, warning=FALSE}
# 1. All variable with similar high unique values
n_cols <- names(imputed_data)[sapply(imputed_data, is.numeric)]
n_cols <- n_cols[n_cols != "Id"]
df_of_interest <- imputed_data[,c(n_cols)]

threshold <- 40
df_cols <- sapply(df_of_interest, function(x) length(unique(x)) > threshold)
df_of_interest <- df_of_interest[, df_cols]

names(df_of_interest)

res.out = Moutlier(df_of_interest[, !(names(df_of_interest) %in% c("MasVnrArea","BsmtFinSF1", "BsmtFinSF2", "X2ndFlrSF", "WoodDeckSF", "EnclosedPorch", "ScreenPorch"))], quantile = 0.9995, col="green")

which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff))
length(which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff))) #44
par(mfrow=c(1,1))
plot( res.out$md, res.out$rd )
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")

# take off m outliers
#imputed_data <- imputed_data[-which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff)),]

```
```{r, results='hide'}
# 2. Generally the houses built latetly are the ones with most quality
res.out <- Moutlier(imputed_data[,c("YearBuilt","OverallQual")], quantile = 0.9995, col="green")
which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff))
par(mfrow=c(1,1))
plot( res.out$md, res.out$rd )
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")

# 3. Generally the houses with more area are the ones with more rooms
res.out <- Moutlier(imputed_data[,c("GrLivArea","TotRmsAbvGrd")], quantile = 0.9995, col="green")
which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff))
par(mfrow=c(1,1))
plot( res.out$md, res.out$rd )
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")

# 4. Generally the houses with more total area are the ones with more garage area also
res.out <- Moutlier(imputed_data[,c("GarageArea","TotalBsmtSF")], quantile = 0.9995, col="green")
which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff))
par(mfrow=c(1,1))
plot( res.out$md, res.out$rd )
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")

# 5. Moutliers about total areas zones
res.out <- Moutlier(imputed_data[,c("X1stFlrSF","TotalBsmtSF","GrLivArea")], quantile = 0.9995, col="green") #GarageArea, si se lo sumo hay menos outliers
which((res.out$md > res.out$cutoff)&(res.out$rd > res.out$cutoff))
par(mfrow=c(1,1))
plot( res.out$md, res.out$rd )
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")

```

# Profiling
As we have declared before in the target variable exploration, the distribution is not normal. To test for autocorrelation the acf() function is used.

```{r}
acf(imputed_data$SalePrice)
ks.test(imputed_data$SalePrice, 'pnorm', mean(imputed_data$SalePrice), sd(imputed_data$SalePrice))

ggplot(data=imputed_data, aes(SalePrice, y = after_stat(density))) +
  geom_histogram(breaks=seq(0, max(imputed_data$SalePrice), by=5000),col='lightblue',fill='steelblue') +
  geom_density(lwd=1,col='red') +labs(title="Histogram for price with density", x="SalePrice", y="Count")
```
## Variables with most association with the response variable
For the quantitative variables there are some variable that are highly significant positively correlated (r > 0.50, p ~ 0) to the target variable, SalePrice:  OverallQual, GrLivArea, GarageCars, GarageArea, TotalBsmtSF, X1stFlrSF, FullBath, TotRmsAbvGrd, YearBuilt and YearRemodAdd. This seems logical as the higher the overall condition rating of the house is and the bigger the area above grade living, the more it would cost. There isn't any variable which is highly significant negatively correlated to the SalePrice (r < -0.50 p = 0).
For the qualitative variables it is clear that the Neighborhood explains the most variance in the SalePrice variable (R^2 = 0.546, p ~ 0).  The influence of the other qualitative variables (with R^2 > 0.3) are in order: Neighborhood, ExterQual, BsmtQual, KitchenQual, f.GarageArea, GarageFinish. EnclosedPorch_binary, Secondfloor and Pool_binary are poorly associated as they have a R^2-value under 4%.
```{r, results='hide'}
res.con <- condes(imputed_data,35) #names(imputed_data)[35] - [1] "SalePrice"
res.con$quanti
res.con$quali
```
# Modeling
To create a model, first we split the train data (imputed_data) into train_new and test_new. We need the test_new to validate the accuracy of the model with data that has the target variable and the model has not used, to check if the model has overfitting.
```{r}
set.seed(123)
rows <- sample(nrow(imputed_data), .7 * nrow(imputed_data))
train_new  <- imputed_data[rows, ]
test_new <- imputed_data[-rows, ]
```
Before building the model, we should first study the multicolinearity and check if there are any predictors that are highly correlated. 
We have seen that the previous variables are very correlated, GarageArea with GarageCars with 88% correlation,  TotRmsAbvGrd and GrLivArea with 83% correlation,  X1stFlrSF and TotalBsmtSF with 81% correlation and SalePrice with OverallQual with 79% what is indeed our target variable. We will not, for now, in the modeling process consider excluding some of the variables that are very correlated. 
```{r}
num_cols <- names(train_new)[sapply(train_new, is.numeric)]
num_data<-train_new[,c(num_cols)]
descrCor <-  cor(num_data)
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > .99) 

max_cor_value <-max(abs(descrCor[upper.tri(descrCor)])) # maximum correlated
which(descrCor == max_cor_value, arr.ind = TRUE) # garageArea and garagecars with 88% correlation

vec <- as.vector(descrCor)
sort(unique(vec), decreasing = TRUE)[3];which(descrCor == sort(unique(vec), decreasing = TRUE)[3], arr.ind = TRUE)
sort(unique(vec), decreasing = TRUE)[4];which(descrCor == sort(unique(vec), decreasing = TRUE)[4], arr.ind = TRUE) 
sort(unique(vec), decreasing = TRUE)[5];which(descrCor == sort(unique(vec), decreasing = TRUE)[5], arr.ind = TRUE) 
```
## Build different models with their metrics
We start by building the model with a manual stepwise technique, we will check the R^2, RSS and AIC metrics to select the better model. We can start by adding the more correlated numerical variables to the target variable. 
```{r echo = T, results = 'hide'}
display(m0 <- lm(SalePrice ~ 1, data = train_new))
display(m0 <- lm(log(SalePrice) ~ 1, data = train_new))
display(m1 <- lm(log(SalePrice) ~ OverallQual , data = train_new))
#plot(m1)
anova(m0,m1) # rss
as.matrix(AIC(m0, m1)) 

display(m2 <- lm(log(SalePrice) ~ OverallQual*log(GrLivArea), data = train_new)) # with interaction
display(m2 <- lm(log(SalePrice) ~ OverallQual+log(GrLivArea), data = train_new)) # without interaction
#plot(m2)
anova(m1,m2)
as.matrix(AIC(m1, m2))

display(m3 <- lm(log(SalePrice) ~OverallQual+log(GrLivArea)*GarageCars , data = train_new))
display(m3 <- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars , data = train_new))
anova(m2,m3) 
as.matrix(AIC(m2, m3)) 
summary(m3)

display(m4 <- lm(log(SalePrice)~OverallQual+log(GrLivArea)+GarageCars+GarageArea , data = train_new))
anova(m3,m4)
as.matrix(AIC(m3, m4))
summary(m4)

display(m5<- lm(log(SalePrice) ~OverallQual +log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF , data = train_new))
anova(m4,m5)
summary(m5) 
as.matrix(AIC(m4, m5)) 

display(m6<- lm(log(SalePrice) ~OverallQual +log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+ X1stFlrSF, data = train_new))
anova(m5,m6)
summary(m6) 
as.matrix(AIC(m5, m6))  #does not improve

display(m7<- lm(log(SalePrice) ~OverallQual +log(GrLivArea)+GarageCars+TotalBsmtSF+FullBath, data = train_new))
anova(m5,m7)  # does not improve much
summary(m7) 
as.matrix(AIC(m5, m7)) 

display(m8<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+TotRmsAbvGrd, data = train_new))
anova(m5,m8) # not improve
as.matrix(AIC(m7, m8)) 

display(m9<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt, data = train_new))
anova(m5,m9) 
summary(m8)
as.matrix(AIC(m8, m9))  # improves

display(m10<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd, data = train_new))
anova(m9,m10) 
summary(m10)
as.matrix(AIC(m9, m10)) 

display(m11<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+MasVnrArea, data = train_new))
anova(m10,m11) 
summary(m11)
as.matrix(AIC(m10, m11))  # not much better


display(m12<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces, data = train_new))
anova(m10,m12) 
summary(m12) # r^2 better
as.matrix(AIC(m10, m12)) 


display(m13<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1, data = train_new))
anova(m12,m13) 
summary(m13) # r^2 NOT better , rss and AIC yes -> keep
as.matrix(AIC(m10, m13))

display(m14<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1+LotFrontage, data = train_new))
anova(m13,m14) # R^2 bbetter -> keep
summary(m14)
as.matrix(AIC(m13, m14))

display(m15<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1+LotFrontage+WoodDeckSF, data = train_new))
anova(m14,m15) # better
summary(m15) 
as.matrix(AIC(m14, m15))

# delete garagearea bv very correlated to garage
display(m16<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1+LotFrontage+WoodDeckSF+OpenPorchSF, data = train_new))
anova(m15,m16)  # not better
summary(m16) 
as.matrix(AIC(m14, m16))

display(m17<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1+LotFrontage+WoodDeckSF+X2ndFlrSF, data = train_new))
anova(m15,m17)  # not better 
summary(m17) 
as.matrix(AIC(m15, m17))
vif(m17)

display(m18<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1+LotFrontage+WoodDeckSF+HalfBath, data = train_new))
anova(m15,m18)  # not better 
summary(m18) 
as.matrix(AIC(m15, m18))
vif(m18)

```
We stop here, the model we end up with is :
```{r}
display(m15<- lm(log(SalePrice) ~OverallQual+log(GrLivArea)+GarageCars+GarageArea+TotalBsmtSF+YearBuilt+YearRemodAdd+Fireplaces+BsmtFinSF1+LotFrontage+WoodDeckSF, data = train_new))
as.matrix(AIC(m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11, m12, m13, m14, m15))
summary(m15) 
summary(m15)$r.squared
# check if there are any correlated predictors
treshold_vif<-function(rsq){
  vif<- 1/(1-rsq)
  return(vif)
}
treshold_vif(summary(m15)$r.squared)
vif(m15) # there are not
```
We can also use the automatic  forward stepwise step() and see which are the variables that will create a model with higher accuracy.
```{r}
mod.fow <- stats::step(lm(SalePrice~. , data = train_new), trace = FALSE, direction = "forward")
summary(mod.fow)
n00<- lm(log(SalePrice) ~ LotArea + OverallQual + OverallCond + YearBuilt +
                  MasVnrArea + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF +
                  X1stFlrSF + X2ndFlrSF  +
                  BsmtFullBath + FullBath + HalfBath + BedroomAbvGr +
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  OpenPorchSF + X3SsnPorch + ScreenPorch +
                  PoolArea, data = train_new)
#summary(n00)
summary(n00)$r.squared
par(mfrow=c(2,2))
plot(n00)
par(mfrow=c(1,2))
influencePlot(n00)
corrplot(descrCor, method = 'color')

# check if there are any correlated predictors
treshold_vif(summary(n00)$r.squared)
vif(n00) # there are not
which(as.vector(vif(n00))>treshold_vif(summary(n00)$r.squared))# there are not highly correlated variables in the model

# delete with a backwards stepwise : MasVnrArea, FullBath, HalfBath, BedroomAbvGr,OpenPorchSF, X3SsnPorch
n0<- lm(log(SalePrice) ~  LotArea + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea, data = train_new)


```
Now we have 2 potential models, m15 created with mechanic stepwise and n0 with automatic stepwise, we shall check which is better.
```{r}
as.matrix(AIC(m15,n0))
anova(m15,n0)
summary(n0)$r.squared
summary(m15)$r.squared
```
We will proceed with the n0 model. 
##  Plot to show which transformations we shall apply
```{r}
# Check BoxTidwell tests: 
# We guess that the variables with a higher range will have a logaritmic transformation (the area variables)

n0<- lm(log(SalePrice) ~  LotArea + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea, data = train_new)

# we do BoxTidwall test to see which transformations we shall apply in the predictors to make a better model. 
# LotArea
boxTidwell(log(SalePrice) ~ LotArea, ~OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea,data=train_new, max.iter = 70 )

v0<- lm(log(SalePrice) ~  log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+ X1stFlrSF+ X2ndFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea, data = train_new)
# we shall convert the LotArea variable into logaritmic

# X1stFlrSF
boxTidwell(log(SalePrice) ~ X1stFlrSF,~ log(LotArea)+ OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+ X2ndFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea,data=train_new, max.iter = 70 )

v1<- lm(log(SalePrice) ~  log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+ log(X1stFlrSF)+ X2ndFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea, data = train_new)


# X2stFlrSF

boxTidwell(log(SalePrice) ~ I(X2ndFlrSF+0.01),~ log(LotArea)+ OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + BsmtFinSF2+X1stFlrSF +
                  BsmtFullBath + KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea,data=train_new, max.iter = 70 ) # we dont need

#summary(v1)
summary(v1)$r.squared
par(mfrow=c(2,2))
plot(v1)
par(mfrow=c(1,1))
#influencePlot(v1)
which(as.vector(vif(v1))>treshold_vif(summary(v1)$r.squared))# there are not highly correlated variables in the model

```
## Select the best model
 We have been looking on the accuracy of the model, currently on 90% (will depend on the seed), but we need to check if there is overfitting using the test sample. 
Looking at these results, seems that the model has no overfitting. No significant difference on the measures of the model between the train sample and the test.
```{r}
rmse <- function(fitted, actual){
  sqrt(mean((fitted - actual)^2))
}

RSQUARE <- function(predictions, actual_values) {
  ss_residual <- sum((actual_values - predictions)^2)
  ss_total <- sum((actual_values - mean(actual_values))^2)
  rsquare <- 1 - (ss_residual / ss_total)
  return(rsquare)
}


results <- data.frame(Model = c("Model with train_new sample",
                                "Model with test_new sample"),
                      RSQRT = round(c(RSQUARE(exp(fitted(v1)), train_new$SalePrice),
                               RSQUARE(exp(predict(v1, newdata = test_new)), test_new$SalePrice)),2),
                      RMSE = round(c(rmse(exp(fitted(v1)), train_new$SalePrice),
                               rmse(exp(predict(v1, newdata = test_new)), test_new$SalePrice)),2))
results

```

## Add categorical variables to the model
Now we shall proceed to adding the categorical variables for our model creation. We also will consider interactions if needed. 
```{r}
#names(train_new)
b0<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea + Neighborhood
        , data = train_new)
#summary(b0)
summary(b0)$r.squared
anova(v1,b0)
AIC(v1,b0)
treshold_vif(summary(b0)$r.squared)
vif(b0) # High vif-> Highly correlated predictor -> wont add to the model


b1<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual
        , data = train_new)

#summary(b1)
summary(b1)$r.squared
anova(n0,b1)
AIC(n0,b1)
treshold_vif(summary(b1)$r.squared)
vif(b1)
# check interactions
b11<- lm(log(SalePrice) ~ (LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea)*ExterQual
        , data = train_new)
#summary(b11)
summary(b11)$r.squared
anova(b1,b11) # the model is better with interactions but for explainability reasons we will not consider the interactions
AIC(b1,b11)


b2<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual
        , data = train_new)

#summary(b2)
summary(b2)$r.squared
anova(b1,b2)
AIC(b1,b2)
treshold_vif(summary(b2)$r.squared)
vif(b2)
# interactions
b21<- lm(log(SalePrice) ~ (LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual) *BsmtQual
        , data = train_new)
#summary(b21)
summary(b21)$r.squared
anova(b2,b21) #  better with interaction 
AIC(b2,b21)
anova(b11,b21) # but worse than b11


b3<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual
        , data = train_new)

#summary(b3)
summary(b3)$r.squared
anova(b2,b3)
AIC(b2,b3)
treshold_vif(summary(b3)$r.squared)
vif(b3)
#with interactions
b31<- lm(log(SalePrice) ~ (LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual)* KitchenQual
        , data = train_new)

#summary(b31)
summary(b31)$r.squared
anova(b3,b31) #  better model with interactions but we will not consider them for explainability purposes
AIC(b3,b31)

b4<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+ GarageFinish
        , data = train_new)

#summary(b4)
summary(b4)$r.squared
anova(b3,b4) # not better model -> we dont use this variable
AIC(b3,b4)
treshold_vif(summary(b4)$r.squared)
vif(b4)

b5<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual + FireplaceQu
        , data = train_new)

#summary(b5)
summary(b5)$r.squared
anova(b3,b5) # not better model
AIC(b3,b5)
treshold_vif(summary(b5)$r.squared)
vif(b5)

b6<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+Foundation
        , data = train_new)

#summary(b6)
summary(b6)$r.squared
anova(b3,b6)
AIC(b3,b6)
treshold_vif(summary(b6)$r.squared)
vif(b6) # we dont add it because highly correlated

b7<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType
        , data = train_new)

#summary(b7)
summary(b7)$r.squared
anova(b3,b7)
AIC(b3,b7)
treshold_vif(summary(b7)$r.squared)
vif(b7) 
# interactions 
b71<- lm(log(SalePrice) ~ (LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea)*GarageType +ExterQual + BsmtQual + KitchenQual
        , data = train_new)

#summary(b71)
summary(b71)$r.squared
anova(b7,b71) # better model 
AIC(b7,b71)
treshold_vif(summary(b71)$r.squared)

b8<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType+MSSubClass
        , data = train_new)

#summary(b8)
summary(b8)$r.squared
anova(b7,b8) # not better model
AIC(b7,b8)
treshold_vif(summary(b8)$r.squared)
vif(b8) # highly correlated

b9<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType+BsmtFinType1
        , data = train_new)

#summary(b9)
summary(b9)$r.squared
anova(b7,b9)  # not better model
AIC(b7,b9)
treshold_vif(summary(b9)$r.squared)

#f.LotArea
b10<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType+f.LotArea
        , data = train_new)

#summary(b10)
summary(b10)$r.squared
anova(b7,b10) # not better model
AIC(b7,b10)
treshold_vif(summary(b10)$r.squared)
vif(b10) 

b12<- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType+HasPorch_binary
        , data = train_new)

#summary(b11)
summary(b11)$r.squared
anova(b7,b11) # a little better model, but not much, we better keep simpler model
AIC(b7,b11)
AIC(b7,b71,b3,b31,b2,b21,b1,b11)
final_model<-b3
```
## Final model metrics
```{r}
results <- data.frame(Model = c("Model with train_new sample","Model with test_new sample"),
                      RSQRT = round(c(RSQUARE(exp(fitted(final_model)), train_new$SalePrice),
                               RSQUARE(exp(predict(final_model, newdata = test_new)), test_new$SalePrice)),2),
                      RMSE = round(c(rmse(exp(fitted(final_model)), train_new$SalePrice),
                               rmse(exp(predict(final_model, newdata = test_new)), test_new$SalePrice)),2));results
```
## Study the presence of a priori influential data observations
In this case, 26 a priori values where found.
```{r}
mean_hat <- mean(hatvalues(final_model));mean_hat
priori <- which(hatvalues(final_model)>4*mean_hat)
length(priori)
```
## Study the presence of a posteriori influential values

For studying the posteriori influential values, we have looked to cook’s distance outliers, where 3 influent data are found and are removed from the sample.
The best model is then reconstructed on this new data set and compared with its original to see the difference. 
Comparing the previous coefficients and the current coefficients, we can say that the change has no relevant changes in the coefficients. 
The R-squared of the model has increased up to 0.94.

```{r, warning=FALSE}
betas <- as.data.frame(dfbetas(final_model))
betas_cutoff <- 2 / sqrt(dim(train_new)[1]);betas_cutoff
influencePlot(final_model, id=list(n=3, method="noteworthy"))

llcoo <-c("1424","811","1183")
# residual outliers
llres <- which(abs(rstudent(final_model))>qnorm(0.995));length(llres)
llrem <- unique(c(rownames(train_new)[llres],llcoo)); length(llrem)
llremreg<-which(rownames(train_new)%in%llrem);llremreg
train_new<-train_new[-llremreg,]

final_modelp <- lm(log(SalePrice) ~ LotFrontage + log(LotArea) + OverallQual + OverallCond + YearBuilt +
                  BsmtFinSF1  + log(X1stFlrSF)+ X2ndFlrSF  +
                  BsmtFullBath + FullBath   + TotRmsAbvGrd+
                  KitchenAbvGr + Fireplaces + GarageCars  +
                  ScreenPorch + PoolArea +ExterQual + BsmtQual + KitchenQual+GarageType
        , data = train_new)
summary(final_modelp)
final_modelp <- step( final_modelp, k=log(nrow(train_new)))

par(mfrow=c(2,2))
plot(final_modelp)

coef_orig <- coef(final_model)
coef_filt <- coef(final_modelp)
#common variables
var_com <- intersect(names(coef_orig), names(coef_filt))
coef_orig <- coef_orig[var_com]
coef_filt <- coef_filt[var_com]
coef_comparison <- data.frame(Variable = names(coef_orig), Original_coefficient = coef_orig, Filtered_coefficient = coef_filt)
print(coef_comparison)

results <- data.frame(Model = c("Model with train_new sample","Model with test_new sample"),
                      RSQRT = round(c(RSQUARE(exp(fitted(final_modelp)), train_new$SalePrice),
                               RSQUARE(exp(predict(final_modelp, newdata = test_new)), test_new$SalePrice)),2),
                      RMSE = round(c(rmse(exp(fitted(final_modelp)), train_new$SalePrice),
                               rmse(exp(predict(final_modelp, newdata = test_new)), test_new$SalePrice)),2));results
```

# Test predictions
## Test standarization
```{r}
test<-read.delim("test.csv", sep=',') 
test$Alley[which(is.na(test$Alley))] <- "No alley access"
test$BsmtQual[which(is.na(test$BsmtQual))] <- "No Basement"
test$BsmtCond[which(is.na(test$BsmtCond))] <- "No Basement"
test$BsmtExposure[which(is.na(test$BsmtExposure))] <- "No Basement"
test$BsmtFinType1[which(is.na(test$BsmtFinType1))] <- "No Basement"
test$BsmtFinType2[which(is.na(test$BsmtFinType2))] <- "No Basement"
test$FireplaceQu[which(is.na(test$FireplaceQu))] <- "No Fireplace"
test$GarageType[which(is.na(test$GarageType))] <- "No Garage"
test$GarageFinish[which(is.na(test$GarageFinish))] <- "No Garage"
test$GarageCond[which(is.na(test$GarageCond))] <- "No Garage"
test$PoolQC[which(is.na(test$PoolQC))] <- "No Pool"
test$Fence[which(is.na(test$Fence))] <- "No Fence"
test$MiscFeature[which(is.na(test$MiscFeature))] <- "None"
char_cols <- which(sapply(test, is.character))
test[, char_cols] <- lapply(test[, char_cols], as.factor) 
test$MSSubClass <- factor(test$MSSubClass)
test$MoSold <- factor(test$MoSold)
vect<- c("Neighborhood", "ExterQual", "BsmtQual", "KitchenQual", "GarageFinish", "FireplaceQu", "Foundation", "GarageType", "MSSubClass", "BsmtFinType1")
factor_test<-test[,c(vect)] # select the first 10 factor variables that are more related with the target
num_cols <- names(test)[sapply(test, is.numeric)]
test2 <- test[,c(num_cols, vect)]

## NEW FEATURES
# - EnclosedPorch_binary: 0 = NO enclosed porch area / 1 = they have
test2$EnclosedPorch_binary<-0
test2$EnclosedPorch_binary[which(test2$EnclosedPorch!=0)]<-1
test2$EnclosedPorch_binary<-as.factor(test2$EnclosedPorch_binary)

# - OpenPorch_binary: 0 = NO open porch area / 1 = they have
test2$OpenPorch_binary<-0
test2$OpenPorch_binary[which(test2$OpenPorchSF!=0)]<-1
test2$OpenPorch_binary<-as.factor(test2$OpenPorch_binary)

# - HasPorch_binary: 0 = NO porch area / 1 = they have (mixing both binary variables)
test2$HasPorch_binary<-0
test2$HasPorch_binary[which(test2$EnclosedPorch!=0 | test2$OpenPorchSF!=0)]<-1
test2$HasPorch_binary<-as.factor(test2$HasPorch_binary)

# - Pool_binary: 0 = no pool / 1 = pool
test2$Pool_binary<-0
test2$Pool_binary[which(test2$PoolArea!=0)]<-1
test2$Pool_binary<-as.factor(test2$Pool_binary)

# - secondfloor: 0 = not second floor / 1 = has second floor 
test2$secondfloor<-0
test2$secondfloor[which(test2$X2ndFlrSF!=0)]<-1
test2$secondfloor<-factor(test2$secondfloor)

test2$univ_outl_count <- 0 # initialize


columns_to_exclude <- c("Id","univ_outl_count")

# apply the function to each column
for (column_name in colnames(test2)) {
  if (!(column_name %in% columns_to_exclude) && is.numeric(test2[[column_name]])) {
    test2 <- update_outliers_count(test2, column_name)
  }
}

mice_imp<-mice(test2[, !names(test2) %in% "GarageYrBlt"],method = "cart")
imputed_test<-complete(mice_imp)

predictions <- predict(final_model, imputed_test)
predictions <- exp(predictions)
prediccion <- data.frame("id"=imputed_test$Id, "SalePrice"=predictions)
write.table(prediccion, file="SalePrice Prediction", sep="\t", col.names=TRUE, row.names=FALSE)


```



## Interpret the Model
The Residuals vs Fitted plot shows that the residuals follow a good linear pattern, which meets the regression assumptions very well. The Normal Q-Q plot shows that the standard errors are mostly normally distributed with a small amount of deviating prediction at the
upper end of the tail and the beginning of the tail. The scale-location plot shows that homoscedasticity is satisfied as a nearly straight line is obtained. 
The residuals vs leverage plot shows that our model includes some high-leverage (so highly influential in our model) points that deviate significantly (more than 4 standardized residuals away) and asymmetrically from the prediction. An influence plot further confirms this believe.
```{r}
summary(final_model)
par(mfrow=c(2,2))
plot(final_model)
coefficients <- coef(final_model);coefficients
```
We can see the coefficient with largest value (in absolute value) is log(X1stFlrSF) with positive impact, 0.385. Meaning that with all other things being equal(c.p) when the first floor square feet increases, the log-price also increases.  Larger first-floor areas are often associated with more spacious living arrangements, which can contribute to higher property values. 
\n \n
Continuing with the analysis of the coefficients, the variable "BsmtQualNo Basement" is noteworthy due to its substantial negative impact of -0.227. This suggests that houses without a basement tend to have a significant decrease in their sale price, all else being equal. Basements often contribute to the overall living space and functionality of a house, so the absence of this feature appears to have a notable effect on the property's value.
\n \n
The presence of a fireplace, as indicated by the variable "Fireplaces," has a positive coefficient of 0.0248. This suggests that houses with fireplaces tend to have higher predicted sale prices. Fireplaces are often considered desirable features, providing both functional and aesthetic benefits, which can positively impact a property's perceived value.
\n \n
Variables related to kitchen quality, such as "KitchenQualFa" (Kitchen Quality Fair) and "KitchenQualTA" (Kitchen Quality Typical), both have negative coefficients of -0.064 and -0.075, respectively. This indicates that, compared to higher-quality kitchens, houses with fair or typical kitchen quality are associated with lower predicted sale prices. Kitchens are known to be focal points for homebuyers, and a well-appointed kitchen is often considered a positive attribute.
\n \n
In conclusion, the coefficients shed light on various aspects that influence the predicted sale prices of houses in the model. Factors such as the house square feet, the presence of a fireplace, and the quality of the kitchen and exterior all play roles in determining the estimated market value of a property.


## Annex
